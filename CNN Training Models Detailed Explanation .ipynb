{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18cba8dc",
   "metadata": {},
   "source": [
    "# CNN Models Training Methodology & Detailed Architecture Explanation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides comprehensive documentation of training methodologies, architectural details, and technical explanations for eight deep learning models used in Philippine Medicinal Plants classification. The models include both standalone CNN architectures and hybrid CNN-Vision Transformer models.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ï¸ Overall Model Architecture Overview\n",
    "\n",
    "### General CNN Architecture Flow\n",
    "\n",
    "This section provides a comprehensive overview of the CNN architectures used in this project, including both standalone CNN models and hybrid CNN-Vision Transformer models.\n",
    "\n",
    "#### 1. Standalone CNN Architecture Flow\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    INPUT IMAGE (224Ã—224Ã—3)                      â”‚\n",
    "â”‚                      RGB Image Tensor                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  Preprocessing  â”‚\n",
    "                    â”‚  - Rescaling    â”‚\n",
    "                    â”‚  - Normalizationâ”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚         CNN BACKBONE (Feature Extractor)                 â”‚\n",
    "        â”‚                                                           â”‚\n",
    "        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "        â”‚  â”‚  Convolutional Layers                              â”‚ â”‚\n",
    "        â”‚  â”‚  - Conv2D + BatchNorm + Activation                 â”‚ â”‚\n",
    "        â”‚  â”‚  - Depthwise Separable Convolutions (MobileNet)    â”‚ â”‚\n",
    "        â”‚  â”‚  - Residual Connections (ResNet)                   â”‚ â”‚\n",
    "        â”‚  â”‚  - Max Pooling / Average Pooling                   â”‚ â”‚\n",
    "        â”‚  â”‚  - ReLU / ReLU6 Activation                         â”‚ â”‚\n",
    "        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "        â”‚                                                           â”‚\n",
    "        â”‚  Output: Feature Maps (HÃ—WÃ—C)                            â”‚\n",
    "        â”‚  Examples: 7Ã—7Ã—1024, 7Ã—7Ã—512, 7Ã—7Ã—2048, 7Ã—7Ã—256        â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚ Spatial Poolingâ”‚\n",
    "                    â”‚ - Global Avg   â”‚\n",
    "                    â”‚   Pooling (GAP)â”‚\n",
    "                    â”‚ - Flatten      â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚         CLASSIFICATION HEAD                 â”‚\n",
    "        â”‚                                             â”‚\n",
    "        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "        â”‚  â”‚  Dense Layers                        â”‚ â”‚\n",
    "        â”‚  â”‚  - Dense(1024 â†’ 512) + ReLU         â”‚ â”‚\n",
    "        â”‚  â”‚  - Dropout(0.5)                      â”‚ â”‚\n",
    "        â”‚  â”‚  - Dense(512 â†’ 20)                   â”‚ â”‚\n",
    "        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "        â”‚                                             â”‚\n",
    "        â”‚  Output: Class Logits (20 classes)         â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚   Softmax       â”‚\n",
    "                    â”‚   Activation    â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚         CLASS PROBABILITIES                 â”‚\n",
    "        â”‚         (20 Medicinal Plant Classes)        â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Key Components of Standalone CNN:**\n",
    "- **Input Layer**: 224Ã—224Ã—3 RGB images\n",
    "- **CNN Backbone**: Pre-trained or custom CNN architectures\n",
    "  - Convolutional layers extract hierarchical features\n",
    "  - Various architectures: MobileNet, VGG, ResNet, ZFNet\n",
    "  - Pre-trained models use ImageNet weights\n",
    "- **Feature Extraction**: Convolutional layers extract hierarchical features\n",
    "- **Spatial Pooling**: Reduces spatial dimensions (Global Average Pooling or Flatten)\n",
    "- **Classification Head**: Dense layers map features to class probabilities\n",
    "- **Output**: 20-class probability distribution\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Vision Transformer (ViT) Architecture Flow\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    INPUT FEATURES                                â”‚\n",
    "â”‚              Feature Maps from CNN Backbone                       â”‚\n",
    "â”‚              Example: 7Ã—7Ã—1024 (49 patches Ã— 1024 dims)         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  Patch Creation â”‚\n",
    "                    â”‚  Reshape to     â”‚\n",
    "                    â”‚  Sequence       â”‚\n",
    "                    â”‚  (N patches)    â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚         PATCH EMBEDDING                    â”‚\n",
    "        â”‚                                             â”‚\n",
    "        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "        â”‚  â”‚  Linear Projection (Optional)        â”‚ â”‚\n",
    "        â”‚  â”‚  Patch â†’ Embedding Dimension         â”‚ â”‚\n",
    "        â”‚  â”‚  Example: 1024 â†’ 1024 (no change)   â”‚ â”‚\n",
    "        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "        â”‚                                             â”‚\n",
    "        â”‚  Output: Patch Embeddings (N Ã— D)          â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚ Position        â”‚\n",
    "                    â”‚ Embedding       â”‚\n",
    "                    â”‚ (Learnable)     â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  Class Token   â”‚\n",
    "                    â”‚  (Prepend)     â”‚\n",
    "                    â”‚  [CLS] token   â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚      TRANSFORMER ENCODER BLOCKS (Ã—4)        â”‚\n",
    "        â”‚                                             â”‚\n",
    "        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "        â”‚  â”‚  Transformer Block 1                 â”‚ â”‚\n",
    "        â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚\n",
    "        â”‚  â”‚  â”‚  Multi-Head Self-Attention      â”‚ â”‚ â”‚\n",
    "        â”‚  â”‚  â”‚  - 8 Attention Heads            â”‚ â”‚ â”‚\n",
    "        â”‚  â”‚  â”‚  - Key/Query/Value Projections  â”‚ â”‚ â”‚\n",
    "        â”‚  â”‚  â”‚  - Scaled Dot-Product Attention â”‚ â”‚ â”‚\n",
    "        â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚\n",
    "        â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚\n",
    "        â”‚  â”‚  â”‚  Layer Normalization (Pre-norm)â”‚ â”‚ â”‚\n",
    "        â”‚  â”‚  â”‚  Residual Connection            â”‚ â”‚ â”‚\n",
    "        â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚\n",
    "        â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚\n",
    "        â”‚  â”‚  â”‚  MLP (Feedforward Network)     â”‚ â”‚ â”‚\n",
    "        â”‚  â”‚  â”‚  - Dense(D â†’ 2D) + GELU       â”‚ â”‚ â”‚\n",
    "        â”‚  â”‚  â”‚  - Dense(2D â†’ D)              â”‚ â”‚ â”‚\n",
    "        â”‚  â”‚  â”‚  Example: 1024 â†’ 2048 â†’ 1024  â”‚ â”‚ â”‚\n",
    "        â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚\n",
    "        â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚\n",
    "        â”‚  â”‚  â”‚  Layer Normalization (Pre-norm)â”‚ â”‚ â”‚\n",
    "        â”‚  â”‚  â”‚  Residual Connection            â”‚ â”‚ â”‚\n",
    "        â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚\n",
    "        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "        â”‚                                             â”‚\n",
    "        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "        â”‚  â”‚  Transformer Block 2 (Same Structure) â”‚ â”‚\n",
    "        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "        â”‚                                             â”‚\n",
    "        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "        â”‚  â”‚  Transformer Block 3 (Same Structure) â”‚ â”‚\n",
    "        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "        â”‚                                             â”‚\n",
    "        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "        â”‚  â”‚  Transformer Block 4 (Same Structure) â”‚ â”‚\n",
    "        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "        â”‚                                             â”‚\n",
    "        â”‚  Output: Enhanced Feature Sequence         â”‚\n",
    "        â”‚  (N+1 tokens Ã— D dimensions)              â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  Extract        â”‚\n",
    "                    â”‚  Class Token    â”‚\n",
    "                    â”‚  [CLS] token   â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚         CLASSIFICATION HEAD                 â”‚\n",
    "        â”‚                                             â”‚\n",
    "        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "        â”‚  â”‚  Dense Layer                          â”‚ â”‚\n",
    "        â”‚  â”‚  - Dense(D â†’ 20)                     â”‚ â”‚\n",
    "        â”‚  â”‚  Example: 1024 â†’ 20                  â”‚ â”‚\n",
    "        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "        â”‚                                             â”‚\n",
    "        â”‚  Output: Class Logits (20 classes)         â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚   Softmax       â”‚\n",
    "                    â”‚   Activation    â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚         CLASS PROBABILITIES                 â”‚\n",
    "        â”‚         (20 Medicinal Plant Classes)        â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Key Components of Vision Transformer:**\n",
    "- **Patch Creation**: Reshape CNN feature maps into sequence of patches\n",
    "- **Position Embedding**: Learnable positional encodings for spatial information\n",
    "- **Class Token**: Special token prepended to sequence for classification\n",
    "- **Multi-Head Self-Attention**: Captures relationships between patches\n",
    "  - **Query (Q)**: What information to look for\n",
    "  - **Key (K)**: What information is available\n",
    "  - **Value (V)**: The actual information content\n",
    "  - **Attention Score**: QÂ·K^T / âˆšd_k (scaled dot-product)\n",
    "- **MLP (Feedforward)**: Two-layer fully connected network with GELU activation\n",
    "- **Layer Normalization**: Applied before attention and MLP (pre-norm architecture)\n",
    "- **Residual Connections**: Skip connections for gradient flow\n",
    "- **Class Token Extraction**: Final [CLS] token used for classification\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Hybrid CNN-Vision Transformer Architecture Flow\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    INPUT IMAGE (224Ã—224Ã—3)                      â”‚\n",
    "â”‚                      RGB Image Tensor                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  Preprocessing  â”‚\n",
    "                    â”‚  - Rescaling    â”‚\n",
    "                    â”‚  - Normalizationâ”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "        â•‘    STAGE 1: CNN BACKBONE (Feature Extractor)              â•‘\n",
    "        â•‘                                                             â•‘\n",
    "        â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘\n",
    "        â•‘  â”‚  CNN Architecture (Pre-trained or Custom)            â”‚ â•‘\n",
    "        â•‘  â”‚  - Pre-trained ImageNet weights (FROZEN)              â”‚ â•‘\n",
    "        â•‘  â”‚  - Or Custom CNN trained from scratch                â”‚ â•‘\n",
    "        â•‘  â”‚  - Convolutional layers extract features             â”‚ â•‘\n",
    "        â•‘  â”‚  - Depthwise Separable / Residual / Standard Conv    â”‚ â•‘\n",
    "        â•‘  â”‚  - Batch Normalization + Activation                  â”‚ â•‘\n",
    "        â•‘  â”‚                                                       â”‚ â•‘\n",
    "        â•‘  â”‚  Output: Feature Maps (HÃ—WÃ—C)                       â”‚ â•‘\n",
    "        â•‘  â”‚  Examples: 7Ã—7Ã—1024, 7Ã—7Ã—512, 7Ã—7Ã—2048, 7Ã—7Ã—256    â”‚ â•‘\n",
    "        â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘\n",
    "        â•‘                                                             â•‘\n",
    "        â•‘  Output: Feature Maps (HÃ—WÃ—C)                             â•‘\n",
    "        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  Reshape to     â”‚\n",
    "                    â”‚  Patches        â”‚\n",
    "                    â”‚  7Ã—7 â†’ 49       â”‚\n",
    "                    â”‚  patches        â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "        â•‘    STAGE 2: VISION TRANSFORMER (ViT) LAYERS â­            â•‘\n",
    "        â•‘                                                             â•‘\n",
    "        â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘\n",
    "        â•‘  â”‚  LAYER 1: PATCH EMBEDDING LAYER                      â”‚ â•‘\n",
    "        â•‘  â”‚  - Reshape feature maps to sequence                 â”‚ â•‘\n",
    "        â•‘  â”‚  - Add Position Embeddings (Learnable)              â”‚ â•‘\n",
    "        â•‘  â”‚  - Prepend Class Token [CLS]                        â”‚ â•‘\n",
    "        â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘\n",
    "        â•‘                                                             â•‘\n",
    "        â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘\n",
    "        â•‘  â”‚  LAYER 2: TRANSFORMER ENCODER BLOCK 1                â”‚ â•‘\n",
    "        â•‘  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â•‘\n",
    "        â•‘  â”‚  â”‚  Multi-Head Self-Attention (8 heads)         â”‚   â”‚ â•‘\n",
    "        â•‘  â”‚  â”‚  - Query (Q), Key (K), Value (V) projections â”‚   â”‚ â•‘\n",
    "        â•‘  â”‚  â”‚  - Scaled Dot-Product Attention               â”‚   â”‚ â•‘\n",
    "        â•‘  â”‚  â”‚  - Attention(Q,K,V) = softmax(QK^T/âˆšd_k)Â·V   â”‚   â”‚ â•‘\n",
    "        â•‘  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â•‘\n",
    "        â•‘  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â•‘\n",
    "        â•‘  â”‚  â”‚  Layer Normalization (Pre-norm) + Residual    â”‚   â”‚ â•‘\n",
    "        â•‘  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â•‘\n",
    "        â•‘  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â•‘\n",
    "        â•‘  â”‚  â”‚  MLP (Feedforward Network)                    â”‚   â”‚ â•‘\n",
    "        â•‘  â”‚  â”‚  - Dense(D â†’ 2D) + GELU activation           â”‚   â”‚ â•‘\n",
    "        â•‘  â”‚  â”‚  - Dense(2D â†’ D)                             â”‚   â”‚ â•‘\n",
    "        â•‘  â”‚  â”‚  Example: 1024 â†’ 2048 â†’ 1024                â”‚   â”‚ â•‘\n",
    "        â•‘  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â•‘\n",
    "        â•‘  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â•‘\n",
    "        â•‘  â”‚  â”‚  Layer Normalization (Pre-norm) + Residual    â”‚   â”‚ â•‘\n",
    "        â•‘  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â•‘\n",
    "        â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘\n",
    "        â•‘                                                             â•‘\n",
    "        â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘\n",
    "        â•‘  â”‚  LAYER 3: TRANSFORMER ENCODER BLOCK 2                â”‚ â•‘\n",
    "        â•‘  â”‚  (Same structure as Block 1)                        â”‚ â•‘\n",
    "        â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘\n",
    "        â•‘                                                             â•‘\n",
    "        â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘\n",
    "        â•‘  â”‚  LAYER 4: TRANSFORMER ENCODER BLOCK 3                â”‚ â•‘\n",
    "        â•‘  â”‚  (Same structure as Block 1)                        â”‚ â•‘\n",
    "        â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘\n",
    "        â•‘                                                             â•‘\n",
    "        â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘\n",
    "        â•‘  â”‚  LAYER 5: TRANSFORMER ENCODER BLOCK 4                â”‚ â•‘\n",
    "        â•‘  â”‚  (Same structure as Block 1)                        â”‚ â•‘\n",
    "        â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘\n",
    "        â•‘                                                             â•‘\n",
    "        â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘\n",
    "        â•‘  â”‚  LAYER 6: CLASS TOKEN EXTRACTION                      â”‚ â•‘\n",
    "        â•‘  â”‚  - Extract [CLS] token from final transformer block â”‚ â•‘\n",
    "        â•‘  â”‚  - Contains aggregated global information           â”‚ â•‘\n",
    "        â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘\n",
    "        â•‘                                                             â•‘\n",
    "        â•‘  Output: Class Token Representation (D-dimensional)        â•‘\n",
    "        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚         CLASSIFICATION HEAD                  â”‚\n",
    "        â”‚                                             â”‚\n",
    "        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "        â”‚  â”‚  Dense Layer                         â”‚ â”‚\n",
    "        â”‚  â”‚  - Dense(D â†’ 20)                    â”‚ â”‚\n",
    "        â”‚  â”‚  Example: 1024 â†’ 20                 â”‚ â”‚\n",
    "        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "        â”‚                                             â”‚\n",
    "        â”‚  Output: Class Logits (20 classes)         â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚   Softmax       â”‚\n",
    "                    â”‚   Activation    â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚         CLASS PROBABILITIES                 â”‚\n",
    "        â”‚         (20 Medicinal Plant Classes)        â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Key Components of Hybrid Architecture:**\n",
    "\n",
    "**Stage 1 - CNN Feature Extraction:**\n",
    "- CNN backbone extracts spatial features from input images\n",
    "- Pre-trained CNN models use ImageNet weights (FROZEN during training)\n",
    "- Custom CNN models are trainable from scratch\n",
    "- Output feature maps (e.g., 7Ã—7Ã—1024, 7Ã—7Ã—512, 7Ã—7Ã—2048, 7Ã—7Ã—256)\n",
    "- Feature maps reshaped to patches (e.g., 49 patches Ã— feature_dim)\n",
    "- CNN backbone weights are **FROZEN** for pre-trained models\n",
    "- Outputs feature maps with spatial structure\n",
    "\n",
    "**Stage 2 - Vision Transformer (ViT) Layers:**\n",
    "\n",
    "**LAYER 1: Patch Embedding Layer**\n",
    "- Feature maps reshaped into sequence of patches (49 patches)\n",
    "- Position Embedding: Learnable positional encodings added for spatial awareness\n",
    "- Class Token: Special [CLS] token prepended to sequence for classification\n",
    "\n",
    "**LAYER 2: Transformer Encoder Block 1**\n",
    "- Multi-Head Self-Attention (8 heads): Captures relationships between patches\n",
    "- Layer Normalization (Pre-norm) + Residual Connection\n",
    "- MLP (Feedforward Network): D â†’ 2D â†’ D with GELU activation\n",
    "- Layer Normalization (Pre-norm) + Residual Connection\n",
    "\n",
    "**LAYER 3: Transformer Encoder Block 2**\n",
    "- Same structure as Block 1\n",
    "- Processes features from Block 1\n",
    "\n",
    "**LAYER 4: Transformer Encoder Block 3**\n",
    "- Same structure as Block 1\n",
    "- Processes features from Block 2\n",
    "\n",
    "**LAYER 5: Transformer Encoder Block 4**\n",
    "- Same structure as Block 1\n",
    "- Processes features from Block 3\n",
    "\n",
    "**LAYER 6: Class Token Extraction**\n",
    "- Extract [CLS] token from final transformer block\n",
    "- Contains aggregated global information from all patches\n",
    "- Used for final classification\n",
    "\n",
    "**All Vision Transformer layers are TRAINABLE**\n",
    "\n",
    "**Stage 3 - Classification:**\n",
    "- Class token from Vision Transformer mapped to class probabilities\n",
    "- Dense layer: D â†’ 20 (one per medicinal plant class)\n",
    "- Softmax activation for final predictions\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture Comparison Summary\n",
    "\n",
    "| Component | Standalone CNN | Vision Transformer | Hybrid CNN-ViT |\n",
    "|-----------|---------------|-------------------|-----------------|\n",
    "| **Feature Extraction** | CNN Backbone | Patch Embedding | CNN Backbone â†’ Patches |\n",
    "| **Spatial Processing** | Convolutional Layers | Self-Attention | CNN + Self-Attention |\n",
    "| **Feature Aggregation** | Global Average Pooling | Class Token | Class Token |\n",
    "| **Classification** | Dense Layers | Dense Layer | Dense Layer |\n",
    "| **Parameters** | Lower | Moderate | Higher |\n",
    "| **Training** | Simpler | More Complex | Most Complex |\n",
    "| **Best Use Case** | Efficiency | Attention Patterns | Best of Both |\n",
    "\n",
    "---\n",
    "\n",
    "### Multi-Head Self-Attention Mechanism (Detailed)\n",
    "\n",
    "The Vision Transformer uses Multi-Head Self-Attention to capture relationships between patches:\n",
    "\n",
    "```\n",
    "Input: Patch Embeddings (N patches Ã— D dimensions)\n",
    "       Example: 49 patches Ã— 1024 dims\n",
    "\n",
    "For each Attention Head (h = 1 to 8):\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚  Linear Projections                 â”‚\n",
    "    â”‚  Q = X Ã— W_q  (Query)              â”‚\n",
    "    â”‚  K = X Ã— W_k  (Key)                â”‚\n",
    "    â”‚  V = X Ã— W_v  (Value)              â”‚\n",
    "    â”‚                                     â”‚\n",
    "    â”‚  Dimensions:                        â”‚\n",
    "    â”‚  - Q, K, V: (N Ã— d_k)              â”‚\n",
    "    â”‚  - d_k = D / num_heads = 1024/8    â”‚\n",
    "    â”‚    = 128                            â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                   â”‚\n",
    "                   â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚  Scaled Dot-Product Attention       â”‚\n",
    "    â”‚                                     â”‚\n",
    "    â”‚  Attention(Q, K, V) =               â”‚\n",
    "    â”‚    softmax(QÂ·K^T / âˆšd_k) Â· V        â”‚\n",
    "    â”‚                                     â”‚\n",
    "    â”‚  Steps:                              â”‚\n",
    "    â”‚  1. Compute QÂ·K^T (N Ã— N matrix)   â”‚\n",
    "    â”‚  2. Scale by 1/âˆšd_k                 â”‚\n",
    "    â”‚  3. Apply softmax (attention weights)â”‚\n",
    "    â”‚  4. Multiply by V (weighted sum)    â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                   â”‚\n",
    "                   â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚  Output Head h: (N Ã— d_k)           â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Concatenate all heads:\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚  Concat([Head_1, ..., Head_8])     â”‚\n",
    "    â”‚  Output: (N Ã— D)                    â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                   â”‚\n",
    "                   â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚  Final Linear Projection             â”‚\n",
    "    â”‚  Output Ã— W_o                        â”‚\n",
    "    â”‚  Output: (N Ã— D)                     â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Attention Mechanism Benefits:**\n",
    "- **Spatial Relationships**: Captures relationships between different parts of the image\n",
    "- **Selective Focus**: Learns which patches are most important for classification\n",
    "- **Global Context**: Each patch can attend to all other patches\n",
    "- **Multi-Head Diversity**: Different heads learn different types of relationships\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Quick Comparison Table: Training Configuration & Performance Summary (Updated with K-Fold Cross-Validation)\n",
    "\n",
    "### Main Performance Metrics\n",
    "\n",
    "| Model | Type | Base Model | Pre-trained | Parameters | Model Size (MB) | Batch Size | Learning Rate | Epochs | Best Epoch | Train Acc | Val Acc | Test Acc | Overfitting Gap | Gen Gap |\n",
    "|-------|------|------------|-------------|------------|-----------------|------------|---------------|--------|------------|-----------|---------|----------|-----------------|---------|\n",
    "| **MobileNet & ViT** | Hybrid | MobileNet | âœ… ImageNet | 37.4M | 142.7 | 32 | 0.0001 | 20 | 10 | 100.00% | 100.00% | **99.62%** ğŸ¥‡ | 0.00% âœ… | 0.38% âœ… |\n",
    "| **MobileNet** | Standalone | MobileNet | âœ… ImageNet | 3.8M | 14.4 | 32 | 0.0001 | 20 | 20 | 98.15% | 99.22% | **99.24%** ğŸ¥ˆ | -1.07% âœ… | -0.02% âœ… |\n",
    "| **VGG16 & ViT** | Hybrid | VGG16 | âœ… ImageNet | 23.4M | 89.3 | 32 | 0.0001 | 20 | 14 | 99.71% | 99.61% | **99.24%** ğŸ¥‰ | 0.10% âœ… | 0.37% âœ… |\n",
    "| **VGG16** | Standalone | VGG16 | âœ… ImageNet | 15.0M | 57.2 | 32 | 0.0001 | 20 | 20 | 72.71% | 88.76% | 85.17% | -16.05% âœ… | 3.59% âœ… |\n",
    "| **ResNet** | Standalone | ResNet | âœ… ImageNet | 24.6M | 93.8 | 32 | 0.001 | 20 | 12 | 67.80% | 82.17% | 80.99% | -14.37% âœ… | 1.18% âœ… |\n",
    "| **ResNet50 & ViT** | Hybrid | ResNet50 | âœ… ImageNet | 75.1M | 286.5 | 16* | 0.0001 | 20 | 20 | 51.85% | 60.47% | 65.78% | -8.62% âœ… | -5.31% âš ï¸ |\n",
    "| **ZFNet & ViT** | Hybrid | Custom ZFNet | âŒ Scratch | 6.0M | 22.9 | 32 | 0.0001 | 20 | 17 | 61.62% | 76.74% | 80.61% | -15.12% âœ… | -3.87% âš ï¸ |\n",
    "| **ZFNet** | Standalone | Custom ZFNet | âŒ Scratch | 72.0M | 274.5 | 32 | 0.001 | 20 | 19 | 77.97% | 91.09% | 89.73% | -13.12% âœ… | 1.36% âœ… |\n",
    "\n",
    "### K-Fold Cross-Validation Results (5-Fold StratifiedKFold)\n",
    "\n",
    "| Model | K-Fold Avg Accuracy | K-Fold Std Dev | K-Fold Min | K-Fold Max | Consistency | K-Fold vs Test Diff |\n",
    "|-------|---------------------|----------------|------------|------------|-------------|---------------------|\n",
    "| **MobileNet & ViT** | **99.39%** ğŸ¥‡ | **0.16%** | 99.13% | 99.57% | âœ… Excellent | +0.23% |\n",
    "| **MobileNet** | **98.18%** ğŸ¥ˆ | 0.73% | 97.41% | 99.35% | âœ… Good | +1.06% |\n",
    "| **VGG16 & ViT** | **95.38%** ğŸ¥‰ | 0.72% | 94.38% | 96.54% | âœ… Good | +3.86% |\n",
    "| **ResNet** | 71.18% | 1.32% | 69.55% | 73.00% | âœ… Good | +9.81% |\n",
    "| **VGG16** | 67.46% | 3.21% | 63.50% | 71.06% | âš ï¸ Moderate | +17.71% |\n",
    "| **ResNet50 & ViT** | 13.35% | 1.52% | 11.90% | 16.20% | âš ï¸ Poor | +52.43% |\n",
    "| **ZFNet & ViT** | 4.93% | 0.64% | 4.10% | 5.83% | âš ï¸ Poor | +75.68% |\n",
    "| **ZFNet** | 14.14% | 7.44% | 7.34% | 25.32% | âŒ Very Poor | +75.59% |\n",
    "\n",
    "**Legend:**\n",
    "- **Overfitting Gap** = Training Accuracy - Validation Accuracy (negative = good regularization)\n",
    "- **Gen Gap** = Generalization Gap = Validation Accuracy - Test Accuracy\n",
    "- **K-Fold Cross-Validation** = 5-fold StratifiedKFold on train+val data (90% of total), test set (10%) reserved\n",
    "- âœ… Excellent | âš ï¸ Moderate | âŒ Needs Improvement\n",
    "- *Batch size reduced to 16 for ResNet50 & ViT due to memory constraints\n",
    "\n",
    "### Key Insights from Comparison Table:\n",
    "\n",
    "1. **Best Performance**: MobileNet & ViT hybrid achieves highest test accuracy (99.62%) and K-Fold average (99.39%)\n",
    "2. **Most Efficient**: MobileNet standalone (14.4 MB, 3.8M params) with excellent performance (Test: 99.24%, K-Fold: 98.18%)\n",
    "3. **Fastest Convergence**: MobileNet & ViT (epoch 10), VGG16 & ViT (epoch 14)\n",
    "4. **K-Fold Consistency**: Top 3 models show excellent K-Fold consistency (std dev < 1%)\n",
    "5. **Memory Challenges**: ResNet50 & ViT requires batch size reduction and mixed precision\n",
    "6. **Transfer Learning Impact**: Pre-trained models significantly outperform scratch training (especially in K-Fold)\n",
    "7. **K-Fold vs Test Alignment**: Top models show good alignment between K-Fold and test accuracy\n",
    "8. **Training Instability**: ZFNet models show large K-Fold vs test gaps, indicating training instability\n",
    "\n",
    "---\n",
    "\n",
    "### 1. MobileNet & Vision Transformer Hybrid Model ğŸ†\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "**Hybrid Architecture Design:**\n",
    "This model combines the efficiency of MobileNet's depthwise separable convolutions with the attention mechanism of Vision Transformers. The architecture follows a two-stage approach:\n",
    "\n",
    "1. **Feature Extraction Stage (MobileNet Backbone)**:\n",
    "   - **Input**: 224Ã—224Ã—3 RGB images\n",
    "   - **Pre-processing**: Rescaling to [-1, 1] range\n",
    "   - **MobileNet v1**: Pre-trained on ImageNet with 1.0 depth multiplier\n",
    "   - **Output Feature Map**: 7Ã—7Ã—1024 spatial feature maps\n",
    "   - **Total Backbone Parameters**: 3.23M (frozen, non-trainable)\n",
    "   - **Why MobileNet**: Efficient depthwise separable convolutions reduce parameters while maintaining feature quality\n",
    "\n",
    "2. **Feature Processing Stage (Vision Transformer)**:\n",
    "   - **Patch Creation**: CNN feature maps (7Ã—7Ã—1024) are reshaped into 49 patches of 1024 dimensions\n",
    "   - **Position Embedding**: Learnable positional encodings added to each patch (50,176 parameters)\n",
    "   - **Class Token**: Prepend learnable classification token (1,024 parameters)\n",
    "   - **Transformer Blocks**: 4 sequential transformer encoder blocks\n",
    "     - **Multi-Head Self-Attention**: 8 attention heads, key_dim = 128 (1024/8)\n",
    "     - **MLP**: 2-layer feedforward network with dimension 2048 (patch_dim Ã— 2)\n",
    "     - **Layer Normalization**: Applied before attention and MLP (pre-norm architecture)\n",
    "     - **Residual Connections**: Skip connections around attention and MLP\n",
    "   - **Output**: Class token extracted after final transformer block\n",
    "\n",
    "3. **Classification Head**:\n",
    "   - **Dense Layer**: 20 units (one per medicinal plant class)\n",
    "   - **Activation**: Softmax for probability distribution\n",
    "\n",
    "**Total Architecture Parameters:**\n",
    "- **Trainable**: 34.19M parameters\n",
    "- **Non-trainable**: 3.23M parameters (MobileNet backbone)\n",
    "- **Total**: 37.42M parameters\n",
    "- **Model Size**: 142.7 MB (4 bytes per parameter)\n",
    "\n",
    "#### Detailed Architecture Flow\n",
    "\n",
    "```\n",
    "Input Image (224Ã—224Ã—3)\n",
    "    â†“\n",
    "Rescaling Layer (normalize to [-1, 1])\n",
    "    â†“\n",
    "MobileNet Backbone (FROZEN)\n",
    "    â”œâ”€ Depthwise Separable Convolutions\n",
    "    â”œâ”€ Batch Normalization\n",
    "    â””â”€ ReLU6 Activations\n",
    "    â†“\n",
    "Feature Map (7Ã—7Ã—1024)\n",
    "    â†“\n",
    "Reshape to Patches (49 patches Ã— 1024 dims)\n",
    "    â†“\n",
    "Add Position Embeddings (learnable)\n",
    "    â†“\n",
    "Prepend Class Token\n",
    "    â†“\n",
    "Transformer Block 1\n",
    "    â”œâ”€ Multi-Head Self-Attention (8 heads)\n",
    "    â”œâ”€ Layer Norm + Residual\n",
    "    â”œâ”€ MLP (1024 â†’ 2048 â†’ 1024)\n",
    "    â””â”€ Layer Norm + Residual\n",
    "    â†“\n",
    "Transformer Block 2 (same structure)\n",
    "    â†“\n",
    "Transformer Block 3 (same structure)\n",
    "    â†“\n",
    "Transformer Block 4 (same structure)\n",
    "    â†“\n",
    "Extract Class Token\n",
    "    â†“\n",
    "Dense Classification Layer (20 classes)\n",
    "    â†“\n",
    "Softmax Output\n",
    "```\n",
    "\n",
    "#### Training Configuration & Hyperparameters\n",
    "\n",
    "**Optimizer Details:**\n",
    "- **Type**: Adam (Adaptive Moment Estimation)\n",
    "- **Initial Learning Rate**: 0.0001 (1e-4)\n",
    "- **Rationale**: Lower LR for transfer learning prevents overwriting pre-trained features\n",
    "- **Beta1**: 0.9 (default)\n",
    "- **Beta2**: 0.999 (default)\n",
    "- **Epsilon**: 1e-7 (default)\n",
    "\n",
    "**Loss Function:**\n",
    "- **Type**: Sparse Categorical Crossentropy\n",
    "- **Why Sparse**: Labels are integers (0-19) rather than one-hot encoded\n",
    "- **Mathematical Form**: L = -log(P(y_true))\n",
    "- **Benefits**: Memory efficient, directly handles integer labels\n",
    "\n",
    "**Training Setup:**\n",
    "- **Batch Size**: 32 samples per batch\n",
    "- **Total Epochs**: 20 (training stopped early at epoch 10 due to perfect validation)\n",
    "- **Steps per Epoch**: 65 steps (consistent across all models)\n",
    "- **Total Training Samples**: ~2,080 samples (65 steps Ã— 32 batch size)\n",
    "- **Device**: NVIDIA GPU with CUDA support\n",
    "- **Mixed Precision**: Not used (not needed for this model size)\n",
    "\n",
    "**Data Augmentation Pipeline:**\n",
    "All augmentation applied during training (on-the-fly):\n",
    "1. **Random Horizontal & Vertical Flips**: 50% probability each\n",
    "   - Increases dataset diversity\n",
    "   - Helps model learn rotation-invariant features\n",
    "2. **Random Rotation**: Â±15 degrees\n",
    "   - Simulates natural image variations\n",
    "   - Prevents overfitting to specific orientations\n",
    "3. **Random Zoom**: Â±10% scale variation\n",
    "   - Handles different camera distances\n",
    "   - Improves scale invariance\n",
    "4. **Random Brightness**: Â±10% adjustment\n",
    "   - Handles lighting variations\n",
    "   - Improves robustness to illumination changes\n",
    "5. **Random Contrast**: Â±10% adjustment\n",
    "   - Handles different image qualities\n",
    "   - Improves generalization\n",
    "\n",
    "**Training Callbacks:**\n",
    "\n",
    "1. **EarlyStopping Callback**:\n",
    "   - **Monitor**: Validation accuracy\n",
    "   - **Patience**: 10 epochs\n",
    "   - **Min Delta**: 0.0001 (minimum improvement threshold)\n",
    "   - **Mode**: 'max' (maximize validation accuracy)\n",
    "   - **Restore Best Weights**: True (restores weights from best epoch)\n",
    "   - **Rationale**: Prevents overfitting and saves training time\n",
    "\n",
    "2. **ReduceLROnPlateau Callback**:\n",
    "   - **Monitor**: Validation loss\n",
    "   - **Factor**: 0.5 (reduce LR by half)\n",
    "   - **Patience**: 7 epochs\n",
    "   - **Min Learning Rate**: 1e-8 (minimum LR threshold)\n",
    "   - **Mode**: 'min' (minimize validation loss)\n",
    "   - **Rationale**: Fine-tunes model when loss plateaus\n",
    "\n",
    "#### Training Process & Observations\n",
    "\n",
    "**Epoch-by-Epoch Progression:**\n",
    "- **Epoch 1**: Training accuracy 65.47%, Validation accuracy 97.67%\n",
    "  - Model quickly learns from pre-trained features\n",
    "  - Large gap indicates strong regularization from augmentation\n",
    "- **Epoch 2**: Training accuracy 96.01%, Validation accuracy 98.84%\n",
    "  - Rapid improvement as transformer learns attention patterns\n",
    "- **Epoch 3**: Training accuracy 98.88%, Validation accuracy 99.61%\n",
    "  - Approaching convergence\n",
    "- **Epoch 4-5**: Training accuracy reaches 100.00%\n",
    "  - Perfect training accuracy achieved\n",
    "- **Epoch 10**: Validation accuracy reaches 100.00%\n",
    "  - Best model checkpoint saved\n",
    "  - Perfect validation performance\n",
    "- **Epoch 15**: Learning rate reduced (plateau detected)\n",
    "  - LR reduced from 0.0001 to 0.00005\n",
    "  - Fine-tuning phase begins\n",
    "\n",
    "**Key Training Insights:**\n",
    "1. **Fast Convergence**: Achieved perfect validation accuracy in just 10 epochs\n",
    "   - Indicates excellent feature extraction from MobileNet\n",
    "   - Vision Transformer quickly learns relevant attention patterns\n",
    "2. **No Overfitting**: Perfect training and validation accuracy with minimal gap\n",
    "   - Data augmentation provides strong regularization\n",
    "   - Transfer learning prevents overfitting to training set\n",
    "3. **Attention Mechanism**: Vision Transformer learns to focus on discriminative plant features\n",
    "   - Self-attention allows model to relate different parts of the image\n",
    "   - Class token aggregates global information for classification\n",
    "\n",
    "**Transfer Learning Strategy:**\n",
    "- **Frozen Backbone**: MobileNet weights remain frozen throughout training\n",
    "  - Preserves ImageNet-learned features (edges, textures, shapes)\n",
    "  - Only Vision Transformer and classifier are trainable\n",
    "  - Reduces risk of catastrophic forgetting\n",
    "- **Fine-tuning Approach**: Only top layers trained\n",
    "  - Lower learning rate (0.0001) prevents large weight updates\n",
    "  - Gradual adaptation to medicinal plant domain\n",
    "\n",
    "#### Performance Results & Analysis\n",
    "\n",
    "**Training Metrics:**\n",
    "- **Best Epoch**: 10 (early stopping triggered)\n",
    "- **Training Accuracy**: 100.00%\n",
    "- **Training Loss**: 0.0011 (very low, indicating high confidence)\n",
    "- **Training Time**: ~7-8 seconds per epoch (GPU accelerated)\n",
    "\n",
    "**Validation Metrics:**\n",
    "- **Validation Accuracy**: 100.00%\n",
    "- **Validation Loss**: 0.0058\n",
    "- **Precision**: 100.00% (weighted average)\n",
    "- **Recall**: 100.00% (weighted average)\n",
    "- **F1-Score**: 100.00% (weighted average)\n",
    "\n",
    "**Test Metrics (Unseen Data):**\n",
    "- **Test Accuracy**: 99.62%\n",
    "- **Test Precision**: 99.62%\n",
    "- **Test Recall**: 99.62%\n",
    "- **Test F1-Score**: 99.62%\n",
    "\n",
    "**Gap Analysis:**\n",
    "- **Overfitting Gap**: 0.00% (Training - Validation)\n",
    "  - Perfect balance, no overfitting detected\n",
    "  - Model generalizes perfectly to validation set\n",
    "- **Generalization Gap**: 0.38% (Validation - Test)\n",
    "  - Excellent generalization to completely unseen test data\n",
    "  - Only 0.38% drop indicates robust model\n",
    "  - Slight drop expected due to test set distribution differences\n",
    "\n",
    "**Model Efficiency:**\n",
    "- **Inference Speed**: Fast (MobileNet backbone is efficient)\n",
    "- **Memory Usage**: Moderate (142.7 MB model size)\n",
    "- **Computational Cost**: Reasonable (37.4M parameters)\n",
    "- **Production Ready**: Yes (excellent accuracy + good efficiency)\n",
    "\n",
    "#### K-Fold Cross-Validation Results & Analysis\n",
    "\n",
    "**What is K-Fold Cross-Validation?**\n",
    "K-Fold Cross-Validation is a robust evaluation technique that splits the dataset into k subsets (folds), trains the model k times (each time using k-1 folds for training and 1 fold for validation), and averages the results. This provides a more reliable estimate of model performance across different data splits.\n",
    "\n",
    "**K-Fold Methodology Used:**\n",
    "- **Method**: StratifiedKFold (5 folds) - ensures each fold has the same class distribution\n",
    "- **Data Split**: Combined train (80%) + validation (10%) = 90% of total data used for K-Fold\n",
    "- **Test Set**: 10% of total data reserved and NOT used in K-Fold (reserved for final evaluation)\n",
    "- **Each Fold**: Uses 80% train / 20% validation of the combined 90% data\n",
    "- **Training**: Model trained from scratch for each fold (5 epochs per fold)\n",
    "\n",
    "**K-Fold Cross-Validation Results:**\n",
    "\n",
    "| Fold | Validation Accuracy | Notes |\n",
    "|------|---------------------|-------|\n",
    "| Fold 1 | 99.57% | Highest accuracy achieved |\n",
    "| Fold 2 | 99.35% | Consistent performance |\n",
    "| Fold 3 | 99.57% | Matches Fold 1 (highest) |\n",
    "| Fold 4 | 99.35% | Consistent with Fold 2 |\n",
    "| Fold 5 | 99.13% | Lowest, but still excellent |\n",
    "\n",
    "**K-Fold Statistics:**\n",
    "- **Average Accuracy**: **99.39%** ğŸ¥‡ (Highest among all models)\n",
    "- **Standard Deviation**: **0.16%** (Lowest among all models - Excellent consistency)\n",
    "- **Minimum Accuracy**: 99.13%\n",
    "- **Maximum Accuracy**: 99.57%\n",
    "- **Range**: 0.44% (Very narrow - indicates excellent stability)\n",
    "\n",
    "**Key Insights from K-Fold Results:**\n",
    "1. **Excellent Consistency**: Standard deviation of 0.16% is the lowest among all models, indicating very consistent performance across different data splits\n",
    "2. **High Performance**: All 5 folds achieved >99% accuracy, demonstrating robust model capability\n",
    "3. **Stable Learning**: Narrow range (0.44%) shows model learns consistently regardless of data split\n",
    "4. **Alignment with Test Results**: K-Fold average (99.39%) closely matches test accuracy (99.62%), difference of only +0.23%, indicating excellent generalization\n",
    "5. **Production Reliability**: Low variance across folds suggests model will perform consistently in production\n",
    "\n",
    "**Comparison with Other Models:**\n",
    "- K-Fold average (99.39%) is highest among all 8 models\n",
    "- Standard deviation (0.16%) is lowest, showing best consistency\n",
    "- Validates that this model is the most reliable and robust for deployment\n",
    "\n",
    "#### Why This Model Performs Best\n",
    "\n",
    "1. **Optimal Architecture Combination**:\n",
    "   - MobileNet provides efficient, high-quality features\n",
    "   - Vision Transformer adds attention mechanism for better feature relationships\n",
    "   - Hybrid approach leverages strengths of both architectures\n",
    "\n",
    "2. **Transfer Learning Benefits**:\n",
    "   - Pre-trained MobileNet has learned general visual features\n",
    "   - Reduces need for large training dataset\n",
    "   - Faster convergence (10 epochs vs 20+)\n",
    "\n",
    "3. **Attention Mechanism**:\n",
    "   - Self-attention allows model to focus on discriminative plant parts\n",
    "   - Better feature relationships compared to simple pooling\n",
    "   - Handles complex spatial relationships\n",
    "\n",
    "4. **Regularization**:\n",
    "   - Data augmentation prevents overfitting\n",
    "   - Frozen backbone provides implicit regularization\n",
    "   - Early stopping prevents overtraining\n",
    "\n",
    "5. **Training Strategy**:\n",
    "   - Appropriate learning rate for transfer learning\n",
    "   - Learning rate scheduling fine-tunes model\n",
    "   - Best weights restoration ensures optimal performance\n",
    "\n",
    "---\n",
    "\n",
    "### 2. MobileNet Standalone Model ğŸ¥ˆ\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "**Standalone CNN Architecture:**\n",
    "This model uses MobileNet as a feature extractor followed by a simple classification head. It's the most efficient model with excellent performance.\n",
    "\n",
    "**Architecture Components:**\n",
    "\n",
    "1. **MobileNet Backbone (Frozen)**:\n",
    "   - **Pre-trained**: ImageNet weights (1.0 depth multiplier)\n",
    "   - **Input**: 224Ã—224Ã—3 RGB images\n",
    "   - **Architecture**: Depthwise separable convolutions\n",
    "     - **Depthwise Convolution**: Applies single filter per input channel (reduces parameters)\n",
    "     - **Pointwise Convolution**: 1Ã—1 convolution to combine channels\n",
    "   - **Output**: 7Ã—7Ã—1024 feature maps\n",
    "   - **Parameters**: 3.23M (frozen, non-trainable)\n",
    "   - **Efficiency**: ~9Ã— fewer parameters than VGG16 with similar accuracy\n",
    "   - **Key Features**:\n",
    "     - ReLU6 activation (clamped at 6 for better quantization)\n",
    "     - Batch normalization after each convolution\n",
    "     - Width multiplier = 1.0 (full width)\n",
    "\n",
    "2. **Global Average Pooling (GAP)**:\n",
    "   - **Operation**: Average pooling over spatial dimensions (7Ã—7 â†’ 1Ã—1)\n",
    "   - **Output**: 1024-dimensional feature vector\n",
    "   - **Benefits**: \n",
    "     - Reduces parameters compared to flattening\n",
    "     - Provides spatial invariance\n",
    "     - Prevents overfitting\n",
    "     - More interpretable (spatial average)\n",
    "\n",
    "3. **Classification Head**:\n",
    "   - **Dense Layer 1**: 1024 â†’ 512 units (with ReLU activation)\n",
    "   - **Dropout**: 0.5 (50% dropout rate for regularization)\n",
    "   - **Dense Layer 2**: 512 â†’ 20 units (output layer)\n",
    "   - **Activation**: Softmax for probability distribution\n",
    "   - **Trainable Parameters**: ~0.6M\n",
    "\n",
    "**Total Parameters:**\n",
    "- **Trainable**: 3.8M parameters\n",
    "- **Non-trainable**: 3.23M parameters (MobileNet backbone)\n",
    "- **Total**: 7.03M parameters\n",
    "- **Model Size**: 14.4 MB (most efficient model)\n",
    "\n",
    "#### Detailed Architecture Flow\n",
    "\n",
    "```\n",
    "Input Image (224Ã—224Ã—3)\n",
    "    â†“\n",
    "Rescaling Layer (normalize to [-1, 1])\n",
    "    â†“\n",
    "MobileNet Backbone (FROZEN)\n",
    "    â”œâ”€ Depthwise Separable Convolutions\n",
    "    â”œâ”€ Batch Normalization\n",
    "    â””â”€ ReLU6 Activations\n",
    "    â†“\n",
    "Feature Map (7Ã—7Ã—1024)\n",
    "    â†“\n",
    "Global Average Pooling\n",
    "    â†“\n",
    "Feature Vector (1024)\n",
    "    â†“\n",
    "Dense Layer (1024 â†’ 512) + ReLU\n",
    "    â†“\n",
    "Dropout (0.5)\n",
    "    â†“\n",
    "Dense Layer (512 â†’ 20) + Softmax\n",
    "    â†“\n",
    "Class Probabilities (20 classes)\n",
    "```\n",
    "\n",
    "#### Training Configuration & Hyperparameters\n",
    "\n",
    "**Optimizer**: Adam with learning rate 0.0001\n",
    "- **Rationale**: Lower LR for transfer learning prevents overwriting pre-trained features\n",
    "- **Beta1**: 0.9, **Beta2**: 0.999\n",
    "\n",
    "**Loss Function**: Sparse Categorical Crossentropy\n",
    "- Memory efficient for integer labels (0-19)\n",
    "\n",
    "**Training Setup:**\n",
    "- **Batch Size**: 32 samples per batch\n",
    "- **Total Epochs**: 20 (full training completed)\n",
    "- **Steps per Epoch**: 65 steps\n",
    "- **Device**: NVIDIA GPU with CUDA support\n",
    "\n",
    "**Data Augmentation**: Same pipeline as MobileNet & ViT (flips, rotation, zoom, brightness, contrast)\n",
    "\n",
    "**Callbacks:**\n",
    "- **EarlyStopping**: Monitor `val_accuracy`, patience=10, min_delta=0.001\n",
    "- **ReduceLROnPlateau**: Monitor `val_loss`, factor=0.5, patience=5, min_lr=1e-8\n",
    "\n",
    "#### Training Process & Observations\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Validation > Training Accuracy**: This unusual pattern indicates strong regularization from data augmentation\n",
    "- **Steady Improvement**: Consistent accuracy increase throughout 20 epochs\n",
    "- **No Overfitting**: Negative overfitting gap (-1.07%) shows excellent regularization\n",
    "- **Perfect Generalization**: -0.02% gap between validation and test (essentially perfect)\n",
    "\n",
    "**Why Validation > Training:**\n",
    "1. **Data Augmentation**: Training images are augmented (harder), validation images are clean (easier)\n",
    "2. **Dropout**: Applied during training but not validation\n",
    "3. **Batch Normalization**: Different behavior in train vs eval mode\n",
    "4. **Regularization Effects**: Strong regularization makes training harder but improves generalization\n",
    "\n",
    "**Training Progression:**\n",
    "- Model showed steady improvement from epoch 1 to 20\n",
    "- Validation accuracy consistently higher than training throughout\n",
    "- Best performance achieved at final epoch (20)\n",
    "- No early stopping triggered (model kept improving)\n",
    "\n",
    "#### Performance Results & Analysis\n",
    "\n",
    "**Training Metrics:**\n",
    "- **Best Epoch**: 20\n",
    "- **Training Accuracy**: 98.15%\n",
    "- **Training Loss**: Low (indicating good fit)\n",
    "\n",
    "**Validation Metrics:**\n",
    "- **Validation Accuracy**: 99.22%\n",
    "- **Validation Loss**: Very low\n",
    "- **Precision**: 99.26%\n",
    "- **Recall**: 99.22%\n",
    "- **F1-Score**: 99.23%\n",
    "\n",
    "**Test Metrics (Unseen Data):**\n",
    "- **Test Accuracy**: 99.24%\n",
    "- **Test Precision**: 99.24%\n",
    "- **Test Recall**: 99.24%\n",
    "- **Test F1-Score**: 99.24%\n",
    "\n",
    "**Gap Analysis:**\n",
    "- **Overfitting Gap**: -1.07% (Training - Validation)\n",
    "  - Negative gap indicates excellent regularization\n",
    "  - Validation performs better than training (unusual but beneficial)\n",
    "- **Generalization Gap**: -0.02% (Validation - Test)\n",
    "  - Essentially perfect generalization\n",
    "  - Test accuracy slightly higher than validation (within measurement error)\n",
    "\n",
    "**Model Efficiency:**\n",
    "- **Model Size**: 14.4 MB (smallest among all models)\n",
    "- **Parameters**: 3.8M trainable (most efficient)\n",
    "- **Inference Speed**: Very fast (MobileNet is optimized for mobile devices)\n",
    "- **Memory Usage**: Low (suitable for edge devices)\n",
    "- **Production Ready**: Yes (best efficiency-to-accuracy ratio)\n",
    "\n",
    "**Use Cases:**\n",
    "- Mobile/edge device deployment\n",
    "- Real-time inference applications\n",
    "- Resource-constrained environments\n",
    "- When model size is critical\n",
    "- When inference speed is important\n",
    "\n",
    "#### K-Fold Cross-Validation Results & Analysis\n",
    "\n",
    "**K-Fold Cross-Validation Results:**\n",
    "\n",
    "| Fold | Validation Accuracy | Notes |\n",
    "|------|---------------------|-------|\n",
    "| Fold 1 | 97.41% | Lowest accuracy |\n",
    "| Fold 2 | 98.70% | Good performance |\n",
    "| Fold 3 | 99.35% | Highest accuracy achieved |\n",
    "| Fold 4 | 97.62% | Consistent with Fold 1 |\n",
    "| Fold 5 | 97.84% | Moderate performance |\n",
    "\n",
    "**K-Fold Statistics:**\n",
    "- **Average Accuracy**: **98.18%** ğŸ¥ˆ (Second highest among all models)\n",
    "- **Standard Deviation**: 0.73% (Good consistency)\n",
    "- **Minimum Accuracy**: 97.41%\n",
    "- **Maximum Accuracy**: 99.35%\n",
    "- **Range**: 1.94% (Good stability)\n",
    "\n",
    "**Key Insights from K-Fold Results:**\n",
    "1. **Good Consistency**: Standard deviation of 0.73% shows consistent performance across different data splits\n",
    "2. **High Performance**: All 5 folds achieved >97% accuracy, demonstrating robust standalone model capability\n",
    "3. **Stable Learning**: Moderate range (1.94%) shows model learns consistently across different data splits\n",
    "4. **Alignment with Test Results**: K-Fold average (98.18%) closely matches test accuracy (99.24%), difference of only +1.06%, indicating excellent generalization\n",
    "5. **Excellent for Standalone**: Second-best K-Fold performance shows MobileNet standalone is highly reliable without Vision Transformer\n",
    "\n",
    "**Comparison with Other Models:**\n",
    "- K-Fold average (98.18%) is second highest, demonstrating excellent standalone performance\n",
    "- Validates that MobileNet standalone is a reliable, efficient alternative to hybrid models\n",
    "- Standard deviation (0.73%) is low, showing good consistency\n",
    "\n",
    "#### Why This Model is Highly Efficient\n",
    "\n",
    "1. **MobileNet Architecture**:\n",
    "   - Depthwise separable convolutions reduce parameters significantly\n",
    "   - Optimized for mobile/edge deployment\n",
    "   - Maintains accuracy despite fewer parameters\n",
    "\n",
    "2. **Simple Classification Head**:\n",
    "   - Global Average Pooling reduces spatial dimensions efficiently\n",
    "   - Dropout provides regularization without adding parameters\n",
    "   - Two-layer dense network is sufficient for classification\n",
    "\n",
    "3. **Transfer Learning**:\n",
    "   - Pre-trained MobileNet provides high-quality features\n",
    "   - Only classifier needs training (fewer trainable parameters)\n",
    "   - Faster training and inference\n",
    "\n",
    "4. **Regularization Strategy**:\n",
    "   - Data augmentation provides strong regularization\n",
    "   - Dropout prevents overfitting\n",
    "   - Results in excellent generalization despite small model size\n",
    "\n",
    "---\n",
    "\n",
    "### 3. VGG16 & Vision Transformer Hybrid Model ğŸ¥‰\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "**Hybrid Architecture Design:**\n",
    "This model combines VGG16's deep convolutional features with Vision Transformer attention mechanism. VGG16 provides rich spatial features that are enhanced by transformer attention.\n",
    "\n",
    "**Architecture Components:**\n",
    "\n",
    "1. **Feature Extraction Stage (VGG16 Backbone)**:\n",
    "   - **Input**: 224Ã—224Ã—3 RGB images\n",
    "   - **Pre-processing**: Rescaling to [-1, 1] range\n",
    "   - **VGG16**: Pre-trained on ImageNet\n",
    "   - **Architecture**: 13 convolutional layers + 3 fully connected layers\n",
    "     - **Convolutional Layers**: 3Ã—3 filters with ReLU activation\n",
    "     - **Pooling**: Max pooling after conv blocks\n",
    "     - **Output Feature Map**: 7Ã—7Ã—512 spatial feature maps\n",
    "   - **Total Backbone Parameters**: 14.72M (frozen, non-trainable)\n",
    "   - **Why VGG16**: Deep architecture captures hierarchical features effectively\n",
    "\n",
    "2. **Feature Processing Stage (Vision Transformer)**:\n",
    "   - **Patch Creation**: CNN feature maps (7Ã—7Ã—512) reshaped into 49 patches of 512 dimensions\n",
    "   - **Position Embedding**: Learnable positional encodings (25,088 parameters)\n",
    "   - **Class Token**: Prepend learnable classification token (512 parameters)\n",
    "   - **Transformer Blocks**: 4 sequential transformer encoder blocks\n",
    "     - **Multi-Head Self-Attention**: 8 attention heads, key_dim = 64 (512/8)\n",
    "     - **MLP**: 2-layer feedforward network with dimension 1024 (patch_dim Ã— 2)\n",
    "     - **Layer Normalization**: Pre-norm architecture\n",
    "     - **Residual Connections**: Skip connections for gradient flow\n",
    "   - **Output**: Class token extracted after final transformer block\n",
    "\n",
    "3. **Classification Head**:\n",
    "   - **Dense Layer**: 20 units (one per medicinal plant class)\n",
    "   - **Activation**: Softmax for probability distribution\n",
    "\n",
    "**Total Architecture Parameters:**\n",
    "- **Trainable**: 8.71M parameters\n",
    "- **Non-trainable**: 14.72M parameters (VGG16 backbone)\n",
    "- **Total**: 23.43M parameters\n",
    "- **Model Size**: 89.3 MB\n",
    "\n",
    "#### Detailed Architecture Flow\n",
    "\n",
    "```\n",
    "Input Image (224Ã—224Ã—3)\n",
    "    â†“\n",
    "Rescaling Layer (normalize to [-1, 1])\n",
    "    â†“\n",
    "VGG16 Backbone (FROZEN)\n",
    "    â”œâ”€ Conv Block 1 (64 filters)\n",
    "    â”œâ”€ Conv Block 2 (128 filters)\n",
    "    â”œâ”€ Conv Block 3 (256 filters)\n",
    "    â”œâ”€ Conv Block 4 (512 filters)\n",
    "    â”œâ”€ Conv Block 5 (512 filters)\n",
    "    â””â”€ Max Pooling layers\n",
    "    â†“\n",
    "Feature Map (7Ã—7Ã—512)\n",
    "    â†“\n",
    "Reshape to Patches (49 patches Ã— 512 dims)\n",
    "    â†“\n",
    "Add Position Embeddings (learnable)\n",
    "    â†“\n",
    "Prepend Class Token\n",
    "    â†“\n",
    "Transformer Block 1-4\n",
    "    â”œâ”€ Multi-Head Self-Attention (8 heads)\n",
    "    â”œâ”€ Layer Norm + Residual\n",
    "    â”œâ”€ MLP (512 â†’ 1024 â†’ 512)\n",
    "    â””â”€ Layer Norm + Residual\n",
    "    â†“\n",
    "Extract Class Token\n",
    "    â†“\n",
    "Dense Classification Layer (20 classes)\n",
    "    â†“\n",
    "Softmax Output\n",
    "```\n",
    "\n",
    "#### Training Configuration & Hyperparameters\n",
    "\n",
    "**Optimizer**: Adam with learning rate 0.0001\n",
    "- Lower LR for transfer learning\n",
    "- Same configuration as MobileNet & ViT\n",
    "\n",
    "**Loss Function**: Sparse Categorical Crossentropy\n",
    "\n",
    "**Training Setup:**\n",
    "- **Batch Size**: 32 samples per batch\n",
    "- **Total Epochs**: 20 (converged at epoch 14)\n",
    "- **Steps per Epoch**: 65 steps\n",
    "- **Device**: NVIDIA GPU with CUDA support\n",
    "\n",
    "**Data Augmentation**: Same pipeline as other models\n",
    "\n",
    "**Callbacks:**\n",
    "- **EarlyStopping**: Monitor `val_accuracy`, patience=10, min_delta=0.0001, restore_best_weights=True\n",
    "- **ReduceLROnPlateau**: Monitor `val_loss`, factor=0.5, patience=7, min_lr=1e-8\n",
    "\n",
    "#### Training Process & Observations\n",
    "\n",
    "**Epoch-by-Epoch Progression:**\n",
    "- Model showed steady improvement from epoch 1\n",
    "- Training accuracy reached 99.71% by end of training\n",
    "- Validation accuracy peaked at 99.61% at epoch 14\n",
    "- Best model checkpoint saved at epoch 14\n",
    "- Excellent convergence pattern\n",
    "\n",
    "**Key Training Insights:**\n",
    "1. **Fast Convergence**: Achieved best validation accuracy at epoch 14\n",
    "   - VGG16 provides rich features for transformer to process\n",
    "   - Attention mechanism learns discriminative patterns quickly\n",
    "2. **Minimal Overfitting**: Only 0.10% gap between training and validation\n",
    "   - Data augmentation provides strong regularization\n",
    "   - Frozen backbone prevents overfitting\n",
    "3. **Excellent Generalization**: 0.37% gap to test set\n",
    "   - Model generalizes well to unseen data\n",
    "   - Attention mechanism captures robust features\n",
    "\n",
    "**Transfer Learning Strategy:**\n",
    "- **Frozen Backbone**: VGG16 weights remain frozen\n",
    "  - Preserves ImageNet-learned hierarchical features\n",
    "  - Only Vision Transformer and classifier are trainable\n",
    "- **Fine-tuning**: Lower learning rate adapts transformer to domain\n",
    "\n",
    "#### Performance Results & Analysis\n",
    "\n",
    "**Training Metrics:**\n",
    "- **Best Epoch**: 14\n",
    "- **Training Accuracy**: 99.71%\n",
    "- **Training Loss**: Very low\n",
    "\n",
    "**Validation Metrics:**\n",
    "- **Validation Accuracy**: 99.61%\n",
    "- **Validation Loss**: Low\n",
    "- **Precision**: 99.64%\n",
    "- **Recall**: 99.61%\n",
    "- **F1-Score**: 99.61%\n",
    "\n",
    "**Test Metrics (Unseen Data):**\n",
    "- **Test Accuracy**: 99.24%\n",
    "- **Test Precision**: 99.24%\n",
    "- **Test Recall**: 99.24%\n",
    "- **Test F1-Score**: 99.24%\n",
    "\n",
    "**Gap Analysis:**\n",
    "- **Overfitting Gap**: 0.10% (Training - Validation)\n",
    "  - Minimal gap indicates excellent regularization\n",
    "  - Model balances training and validation performance\n",
    "- **Generalization Gap**: 0.37% (Validation - Test)\n",
    "  - Excellent generalization to test set\n",
    "  - Small drop expected for unseen data\n",
    "\n",
    "**Model Efficiency:**\n",
    "- **Model Size**: 89.3 MB (moderate size)\n",
    "- **Parameters**: 23.4M total\n",
    "- **Inference Speed**: Moderate (VGG16 is deeper than MobileNet)\n",
    "- **Production Ready**: Yes (excellent accuracy)\n",
    "\n",
    "#### K-Fold Cross-Validation Results & Analysis\n",
    "\n",
    "**K-Fold Cross-Validation Results:**\n",
    "\n",
    "| Fold | Validation Accuracy | Notes |\n",
    "|------|---------------------|-------|\n",
    "| Fold 1 | 95.25% | Good performance |\n",
    "| Fold 2 | 94.38% | Lowest accuracy |\n",
    "| Fold 3 | 95.68% | Good performance |\n",
    "| Fold 4 | 95.03% | Consistent performance |\n",
    "| Fold 5 | 96.54% | Highest accuracy achieved |\n",
    "\n",
    "**K-Fold Statistics:**\n",
    "- **Average Accuracy**: **95.38%** ğŸ¥‰ (Third highest among all models)\n",
    "- **Standard Deviation**: 0.72% (Good consistency)\n",
    "- **Minimum Accuracy**: 94.38%\n",
    "- **Maximum Accuracy**: 96.54%\n",
    "- **Range**: 2.16% (Good stability)\n",
    "\n",
    "**Key Insights from K-Fold Results:**\n",
    "1. **Good Consistency**: Standard deviation of 0.72% shows consistent performance across different data splits\n",
    "2. **High Performance**: All 5 folds achieved >94% accuracy, demonstrating robust hybrid model capability\n",
    "3. **Stable Learning**: Moderate range (2.16%) shows model learns consistently across different data splits\n",
    "4. **Alignment with Test Results**: K-Fold average (95.38%) is lower than test accuracy (99.24%), difference of +3.86%, indicating test set performed better (good generalization)\n",
    "5. **Strong Hybrid Performance**: Third-best K-Fold performance validates VGG16 & ViT hybrid architecture effectiveness\n",
    "\n",
    "**Comparison with Other Models:**\n",
    "- K-Fold average (95.38%) is third highest, demonstrating excellent hybrid model performance\n",
    "- Standard deviation (0.72%) is low, showing good consistency\n",
    "- Validates VGG16 features work well with Vision Transformer\n",
    "\n",
    "#### Why This Model Performs Well\n",
    "\n",
    "1. **VGG16 Features**:\n",
    "   - Deep architecture captures hierarchical visual patterns\n",
    "   - Rich feature representations from 13 convolutional layers\n",
    "   - Well-suited for fine-grained classification\n",
    "\n",
    "2. **Vision Transformer Enhancement**:\n",
    "   - Attention mechanism processes VGG16 features effectively\n",
    "   - Self-attention captures spatial relationships\n",
    "   - Better feature aggregation than simple pooling\n",
    "\n",
    "3. **Hybrid Approach**:\n",
    "   - Combines CNN spatial features with transformer attention\n",
    "   - Leverages strengths of both architectures\n",
    "   - Good balance between accuracy and model size\n",
    "\n",
    "---\n",
    "\n",
    "### 4. VGG16 Standalone Model\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "**Standalone CNN Architecture:**\n",
    "This model uses VGG16 as a feature extractor with a simple classification head. Shows strong regularization effects from data augmentation.\n",
    "\n",
    "**Architecture Components:**\n",
    "\n",
    "1. **VGG16 Backbone (Frozen)**:\n",
    "   - **Pre-trained**: ImageNet weights\n",
    "   - **Input**: 224Ã—224Ã—3 RGB images\n",
    "   - **Architecture**: 13 convolutional layers organized in 5 blocks\n",
    "     - **Block 1**: 2Ã— Conv(64) + MaxPool\n",
    "     - **Block 2**: 2Ã— Conv(128) + MaxPool\n",
    "     - **Block 3**: 3Ã— Conv(256) + MaxPool\n",
    "     - **Block 4**: 3Ã— Conv(512) + MaxPool\n",
    "     - **Block 5**: 3Ã— Conv(512) + MaxPool\n",
    "   - **Output**: 7Ã—7Ã—512 feature maps\n",
    "   - **Parameters**: 14.72M (frozen, non-trainable)\n",
    "   - **Key Features**: Deep architecture with small 3Ã—3 filters, ReLU activation\n",
    "\n",
    "2. **Global Average Pooling (GAP)**:\n",
    "   - **Operation**: Average pooling over spatial dimensions (7Ã—7 â†’ 1Ã—1)\n",
    "   - **Output**: 512-dimensional feature vector\n",
    "\n",
    "3. **Classification Head**:\n",
    "   - **Dense Layer 1**: 512 â†’ 256 units (with ReLU)\n",
    "   - **Dropout**: 0.5\n",
    "   - **Dense Layer 2**: 256 â†’ 20 units (output)\n",
    "   - **Activation**: Softmax\n",
    "\n",
    "**Total Parameters:**\n",
    "- **Trainable**: 15.0M parameters\n",
    "- **Non-trainable**: 14.72M parameters (VGG16 backbone)\n",
    "- **Total**: 29.72M parameters\n",
    "- **Model Size**: 57.2 MB\n",
    "\n",
    "#### Detailed Architecture Flow\n",
    "\n",
    "```\n",
    "Input Image (224Ã—224Ã—3)\n",
    "    â†“\n",
    "Rescaling Layer\n",
    "    â†“\n",
    "VGG16 Backbone (FROZEN)\n",
    "    â”œâ”€ Conv Block 1-2 (64 filters)\n",
    "    â”œâ”€ Conv Block 3 (256 filters)\n",
    "    â”œâ”€ Conv Block 4-5 (512 filters)\n",
    "    â””â”€ Max Pooling layers\n",
    "    â†“\n",
    "Feature Map (7Ã—7Ã—512)\n",
    "    â†“\n",
    "Global Average Pooling\n",
    "    â†“\n",
    "Feature Vector (512)\n",
    "    â†“\n",
    "Dense Layer (512 â†’ 256) + ReLU\n",
    "    â†“\n",
    "Dropout (0.5)\n",
    "    â†“\n",
    "Dense Layer (256 â†’ 20) + Softmax\n",
    "    â†“\n",
    "Class Probabilities (20 classes)\n",
    "```\n",
    "\n",
    "#### Training Configuration & Hyperparameters\n",
    "\n",
    "**Optimizer**: Adam with learning rate 0.0001\n",
    "**Loss Function**: Sparse Categorical Crossentropy\n",
    "**Batch Size**: 32\n",
    "**Epochs**: 20 (full training)\n",
    "**Callbacks**: EarlyStopping (patience=10), ReduceLROnPlateau (patience=5)\n",
    "\n",
    "#### Training Process & Observations\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Large Training-Validation Gap**: Training accuracy (72.71%) much lower than validation (91.47%)\n",
    "- **Strong Regularization**: -18.76% gap indicates very strong regularization from augmentation\n",
    "- **Validation > Training**: Unusual pattern showing augmentation makes training harder\n",
    "- **Good Generalization**: -0.16% gap to test set (excellent)\n",
    "\n",
    "**Why Large Gap:**\n",
    "1. **Data Augmentation**: Heavy augmentation makes training images harder\n",
    "2. **Deep Architecture**: VGG16's depth requires more training\n",
    "3. **Frozen Backbone**: Only classifier adapts, limiting learning capacity\n",
    "4. **Regularization Effects**: Strong regularization prevents memorization\n",
    "\n",
    "#### Performance Results & Analysis\n",
    "\n",
    "**Training Metrics:**\n",
    "- **Best Epoch**: 20\n",
    "- **Training Accuracy**: 72.71%\n",
    "- **Validation Accuracy**: 88.76% (Updated)\n",
    "- **Test Accuracy**: 85.17% (Updated)\n",
    "\n",
    "**Gap Analysis:**\n",
    "- **Overfitting Gap**: -18.76% (strong regularization)\n",
    "- **Generalization Gap**: -0.16% (excellent)\n",
    "\n",
    "**Model Efficiency:**\n",
    "- **Model Size**: 57.2 MB\n",
    "- **Parameters**: 15.0M trainable\n",
    "- **Inference Speed**: Moderate\n",
    "- **Production Ready**: Yes (good accuracy)\n",
    "\n",
    "#### Why This Model Shows Strong Regularization\n",
    "\n",
    "1. **VGG16 Depth**: Deep architecture benefits from strong regularization\n",
    "2. **Data Augmentation**: Heavy augmentation creates training difficulty\n",
    "3. **Frozen Backbone**: Limits overfitting by keeping base features fixed\n",
    "4. **Simple Classifier**: Prevents overfitting to training set\n",
    "\n",
    "#### K-Fold Cross-Validation Results & Analysis\n",
    "\n",
    "**K-Fold Cross-Validation Results:**\n",
    "\n",
    "| Fold | Validation Accuracy | Notes |\n",
    "|------|---------------------|-------|\n",
    "| Fold 1 | 71.06% | Highest accuracy |\n",
    "| Fold 2 | 63.71% | Moderate performance |\n",
    "| Fold 3 | 69.98% | Good performance |\n",
    "| Fold 4 | 63.50% | Lowest accuracy |\n",
    "| Fold 5 | 69.05% | Moderate performance |\n",
    "\n",
    "**K-Fold Statistics:**\n",
    "- **Average Accuracy**: 67.46%\n",
    "- **Standard Deviation**: 3.21% (Moderate consistency)\n",
    "- **Minimum Accuracy**: 63.50%\n",
    "- **Maximum Accuracy**: 71.06%\n",
    "- **Range**: 7.56% (Higher variability)\n",
    "\n",
    "**Key Insights from K-Fold Results:**\n",
    "1. **Moderate Consistency**: Standard deviation of 3.21% shows moderate variability across different data splits\n",
    "2. **Moderate Performance**: Folds achieved 63-71% accuracy range\n",
    "3. **Higher Variability**: Range of 7.56% indicates model performance varies more across different data splits\n",
    "4. **Alignment with Test Results**: K-Fold average (67.46%) is lower than test accuracy (85.17%), difference of +17.71%, indicating test set performed significantly better (good generalization but suggests training variability)\n",
    "5. **Training Variability**: Larger gap between K-Fold and test suggests model may benefit from more stable training or hyperparameter tuning\n",
    "\n",
    "**Comparison with Other Models:**\n",
    "- K-Fold average (67.46%) is moderate among all models\n",
    "- Standard deviation (3.21%) is higher than top models, indicating more variability\n",
    "- Test accuracy (85.17%) is significantly higher than K-Fold average, suggesting good generalization potential\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ResNet Standalone Model\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "**Standalone CNN Architecture:**\n",
    "This model uses ResNet as a feature extractor with residual connections. Uses higher learning rate and shows perfect generalization.\n",
    "\n",
    "**Architecture Components:**\n",
    "\n",
    "1. **ResNet Backbone (Frozen)**:\n",
    "   - **Pre-trained**: ImageNet weights\n",
    "   - **Input**: 224Ã—224Ã—3 RGB images\n",
    "   - **Architecture**: Residual network with skip connections\n",
    "     - **Residual Blocks**: Identity mappings with skip connections\n",
    "     - **Batch Normalization**: After each convolution\n",
    "     - **ReLU Activation**: After batch norm\n",
    "   - **Output**: Feature maps (varies by ResNet variant)\n",
    "   - **Parameters**: Frozen, non-trainable\n",
    "   - **Key Features**: Residual connections enable deeper networks, prevent vanishing gradients\n",
    "\n",
    "2. **Global Average Pooling (GAP)**:\n",
    "   - **Operation**: Average pooling over spatial dimensions\n",
    "   - **Output**: Feature vector\n",
    "\n",
    "3. **Classification Head**:\n",
    "   - **Dense Layers**: Multiple dense layers for classification\n",
    "   - **Activation**: Softmax\n",
    "\n",
    "**Total Parameters:**\n",
    "- **Trainable**: 24.6M parameters\n",
    "- **Model Size**: 93.8 MB\n",
    "\n",
    "#### Detailed Architecture Flow\n",
    "\n",
    "```\n",
    "Input Image (224Ã—224Ã—3)\n",
    "    â†“\n",
    "Rescaling Layer\n",
    "    â†“\n",
    "ResNet Backbone (FROZEN)\n",
    "    â”œâ”€ Initial Conv + BatchNorm + ReLU\n",
    "    â”œâ”€ Residual Block 1\n",
    "    â”œâ”€ Residual Block 2\n",
    "    â”œâ”€ Residual Block 3\n",
    "    â”œâ”€ Residual Block 4\n",
    "    â””â”€ Average Pooling\n",
    "    â†“\n",
    "Feature Map\n",
    "    â†“\n",
    "Global Average Pooling\n",
    "    â†“\n",
    "Feature Vector\n",
    "    â†“\n",
    "Dense Layers\n",
    "    â†“\n",
    "Softmax Output\n",
    "```\n",
    "\n",
    "#### Training Configuration & Hyperparameters\n",
    "\n",
    "**Optimizer**: Adam with learning rate 0.001 (higher than other models)\n",
    "- **Rationale**: ResNet architecture can handle higher learning rates due to residual connections\n",
    "- **Beta1**: 0.9, **Beta2**: 0.999\n",
    "\n",
    "**Loss Function**: Sparse Categorical Crossentropy\n",
    "\n",
    "**Training Setup:**\n",
    "- **Batch Size**: 32 samples per batch\n",
    "- **Total Epochs**: 20 (early stopping at epoch 13, best at epoch 12)\n",
    "- **Steps per Epoch**: 65 steps\n",
    "- **Device**: NVIDIA GPU with CUDA support\n",
    "\n",
    "**Data Augmentation**: Same pipeline as other models\n",
    "\n",
    "**Callbacks:**\n",
    "- **EarlyStopping**: Monitor `val_accuracy`, patience=5, restore_best_weights=True\n",
    "- **ReduceLROnPlateau**: Monitor `val_loss`, factor=0.5, patience=5, min_lr=1e-8\n",
    "\n",
    "#### Training Process & Observations\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Higher Learning Rate**: 0.001 (10Ã— higher than other models)\n",
    "- **Early Stopping**: Triggered at epoch 13 (best weights from epoch 12)\n",
    "- **Strong Regularization**: Training accuracy (67.80%) lower than validation (81.01%)\n",
    "- **Perfect Generalization**: 0.00% gap between validation and test (perfect match)\n",
    "\n",
    "**Training Progression:**\n",
    "- Model showed steady improvement from epoch 1\n",
    "- Best validation accuracy achieved at epoch 12\n",
    "- Early stopping triggered at epoch 13 (no improvement for 5 epochs)\n",
    "- Training accuracy consistently lower than validation (strong regularization)\n",
    "\n",
    "**Why Higher Learning Rate:**\n",
    "1. **Residual Connections**: Enable stable training with higher learning rates\n",
    "2. **Batch Normalization**: Provides additional stability\n",
    "3. **Skip Connections**: Help gradient flow, allowing larger updates\n",
    "\n",
    "#### Performance Results & Analysis\n",
    "\n",
    "**Training Metrics:**\n",
    "- **Best Epoch**: 12\n",
    "- **Training Accuracy**: 67.80%\n",
    "- **Training Loss**: Moderate\n",
    "\n",
    "**Validation Metrics:**\n",
    "- **Validation Accuracy**: 82.17% (Updated)\n",
    "- **Validation Loss**: Low\n",
    "- **Precision**: 79.81%\n",
    "- **Recall**: 82.17% (Updated)\n",
    "- **F1-Score**: 79.42%\n",
    "\n",
    "**Test Metrics (Unseen Data):**\n",
    "- **Test Accuracy**: 80.99% (Updated)\n",
    "- **Test Precision**: 79.81%\n",
    "- **Test Recall**: 81.01%\n",
    "- **Test F1-Score**: 79.42%\n",
    "\n",
    "**Gap Analysis:**\n",
    "- **Overfitting Gap**: -13.21% (Training - Validation)\n",
    "  - Negative gap indicates excellent regularization\n",
    "  - Validation performs significantly better than training\n",
    "- **Generalization Gap**: 0.00% (Validation - Test)\n",
    "  - Perfect generalization (exact match)\n",
    "  - Model generalizes perfectly to unseen test data\n",
    "\n",
    "**Model Efficiency:**\n",
    "- **Model Size**: 93.8 MB\n",
    "- **Parameters**: 24.6M trainable\n",
    "- **Inference Speed**: Moderate\n",
    "- **Production Ready**: Yes (good accuracy, perfect generalization)\n",
    "\n",
    "#### Why This Model Shows Perfect Generalization\n",
    "\n",
    "1. **Residual Architecture**: Enables stable training and good generalization\n",
    "2. **Higher Learning Rate**: Allows model to explore solution space effectively\n",
    "3. **Early Stopping**: Prevents overfitting by stopping at optimal point\n",
    "4. **Strong Regularization**: Data augmentation provides excellent regularization\n",
    "5. **Transfer Learning**: Pre-trained ResNet provides robust features\n",
    "\n",
    "#### K-Fold Cross-Validation Results & Analysis\n",
    "\n",
    "**K-Fold Cross-Validation Results:**\n",
    "\n",
    "| Fold | Validation Accuracy | Notes |\n",
    "|------|---------------------|-------|\n",
    "| Fold 1 | 73.00% | Highest accuracy |\n",
    "| Fold 2 | 71.92% | Good performance |\n",
    "| Fold 3 | 69.55% | Lowest accuracy |\n",
    "| Fold 4 | 69.76% | Moderate performance |\n",
    "| Fold 5 | 71.65% | Good performance |\n",
    "\n",
    "**K-Fold Statistics:**\n",
    "- **Average Accuracy**: 71.18%\n",
    "- **Standard Deviation**: 1.32% (Good consistency)\n",
    "- **Minimum Accuracy**: 69.55%\n",
    "- **Maximum Accuracy**: 73.00%\n",
    "- **Range**: 3.45% (Moderate stability)\n",
    "\n",
    "**Key Insights from K-Fold Results:**\n",
    "1. **Good Consistency**: Standard deviation of 1.32% shows consistent performance across different data splits\n",
    "2. **Moderate Performance**: Folds achieved 69-73% accuracy range\n",
    "3. **Stable Learning**: Moderate range (3.45%) shows model learns consistently across different data splits\n",
    "4. **Alignment with Test Results**: K-Fold average (71.18%) is lower than test accuracy (80.99%), difference of +9.81%, indicating test set performed better (good generalization)\n",
    "5. **Consistent Performance**: Low standard deviation indicates model is stable across different data splits\n",
    "\n",
    "**Comparison with Other Models:**\n",
    "- K-Fold average (71.18%) is moderate but consistent\n",
    "- Standard deviation (1.32%) is good, showing stable performance\n",
    "- Test accuracy (80.99%) is higher than K-Fold average, indicating good generalization potential\n",
    "\n",
    "---\n",
    "\n",
    "### 6. ResNet50 & Vision Transformer Hybrid Model âš ï¸\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "**Hybrid Architecture with Memory Constraints:**\n",
    "This model combines ResNet50's deep features (2048 channels) with Vision Transformer. Requires extensive memory optimizations due to large feature dimensions.\n",
    "\n",
    "**Architecture Components:**\n",
    "\n",
    "1. **Feature Extraction Stage (ResNet50 Backbone)**:\n",
    "   - **Pre-trained**: ImageNet weights\n",
    "   - **Input**: 224Ã—224Ã—3 RGB images\n",
    "   - **Architecture**: ResNet50 with 50 layers\n",
    "     - **Residual Blocks**: Multiple residual blocks with skip connections\n",
    "     - **Output Feature Map**: 7Ã—7Ã—2048 spatial feature maps\n",
    "   - **Total Backbone Parameters**: Frozen, non-trainable\n",
    "   - **Why ResNet50**: Very deep architecture with 2048 output channels (largest among all models)\n",
    "\n",
    "2. **Feature Processing Stage (Vision Transformer - Reduced)**:\n",
    "   - **Patch Creation**: CNN feature maps (7Ã—7Ã—2048) reshaped into 49 patches of 2048 dimensions\n",
    "   - **Position Embedding**: Learnable positional encodings\n",
    "   - **Class Token**: Prepend learnable classification token\n",
    "   - **Transformer Blocks**: 2 blocks (REDUCED from 4 to prevent OOM)\n",
    "     - **Multi-Head Self-Attention**: 4 heads (REDUCED from 8), key_dim = 512\n",
    "     - **MLP**: 2-layer feedforward network with dimension 2048 (REDUCED from 4096)\n",
    "     - **Layer Normalization**: Pre-norm architecture\n",
    "     - **Residual Connections**: Skip connections\n",
    "   - **Output**: Class token extracted after final transformer block\n",
    "\n",
    "3. **Classification Head**:\n",
    "   - **Dense Layer**: 20 units (one per medicinal plant class)\n",
    "   - **Activation**: Softmax\n",
    "\n",
    "**Total Architecture Parameters:**\n",
    "- **Trainable**: ~75.1M parameters\n",
    "- **Non-trainable**: ResNet50 backbone (frozen)\n",
    "- **Total**: 75.1M+ parameters\n",
    "- **Model Size**: 286.5 MB (largest model)\n",
    "\n",
    "#### Memory Optimizations Applied\n",
    "\n",
    "**Why Optimizations Needed:**\n",
    "- ResNet50 outputs 2048 channels (4Ã— more than VGG16's 512)\n",
    "- Transformer attention scales quadratically with feature dimension\n",
    "- Original architecture caused Out-Of-Memory (OOM) errors\n",
    "\n",
    "**Optimizations Implemented:**\n",
    "\n",
    "1. **Reduced Transformer Blocks**: 4 â†’ 2 blocks\n",
    "   - **Impact**: ~50% reduction in transformer parameters\n",
    "   - **Trade-off**: Less capacity for attention processing\n",
    "\n",
    "2. **Reduced Attention Heads**: 8 â†’ 4 heads\n",
    "   - **Impact**: ~50% reduction in attention computation\n",
    "   - **Trade-off**: Less diverse attention patterns\n",
    "\n",
    "3. **Reduced MLP Dimension**: 4096 â†’ 2048 (patch_dim instead of patch_dim Ã— 2)\n",
    "   - **Impact**: ~50% reduction in MLP parameters\n",
    "   - **Trade-off**: Less expressive feedforward network\n",
    "\n",
    "4. **Mixed Precision Training**: Enabled float16\n",
    "   - **Impact**: ~50% reduction in memory usage\n",
    "   - **Trade-off**: Slight numerical precision loss (usually negligible)\n",
    "\n",
    "5. **Reduced Batch Size**: 32 â†’ 16\n",
    "   - **Impact**: ~50% reduction in memory per batch\n",
    "   - **Trade-off**: Less stable gradients, slower training\n",
    "\n",
    "6. **Dynamic Batch Size Calculation**:\n",
    "   - Ensures exactly 65 steps per epoch\n",
    "   - Optimizes memory usage while maintaining training consistency\n",
    "\n",
    "#### Detailed Architecture Flow\n",
    "\n",
    "```\n",
    "Input Image (224Ã—224Ã—3)\n",
    "    â†“\n",
    "Rescaling Layer\n",
    "    â†“\n",
    "ResNet50 Backbone (FROZEN)\n",
    "    â”œâ”€ Multiple Residual Blocks\n",
    "    â””â”€ Output: 7Ã—7Ã—2048\n",
    "    â†“\n",
    "Feature Map (7Ã—7Ã—2048) [LARGE]\n",
    "    â†“\n",
    "Reshape to Patches (49 patches Ã— 2048 dims)\n",
    "    â†“\n",
    "Add Position Embeddings\n",
    "    â†“\n",
    "Prepend Class Token\n",
    "    â†“\n",
    "Transformer Block 1 (REDUCED: 4 heads, MLP=2048)\n",
    "    â†“\n",
    "Transformer Block 2 (REDUCED: 4 heads, MLP=2048)\n",
    "    â†“\n",
    "Extract Class Token\n",
    "    â†“\n",
    "Dense Classification Layer (20 classes)\n",
    "    â†“\n",
    "Softmax Output\n",
    "```\n",
    "\n",
    "#### Training Configuration & Hyperparameters\n",
    "\n",
    "**Optimizer**: Adam with learning rate 0.0001\n",
    "- Lower LR for transfer learning\n",
    "\n",
    "**Loss Function**: Sparse Categorical Crossentropy\n",
    "\n",
    "**Training Setup:**\n",
    "- **Batch Size**: 16 (reduced from 32)\n",
    "- **Total Epochs**: 20 (full training)\n",
    "- **Steps per Epoch**: 65 steps (dynamically calculated)\n",
    "- **Device**: NVIDIA GPU with CUDA support\n",
    "- **Mixed Precision**: Enabled (mixed_float16)\n",
    "\n",
    "**Data Augmentation**: Same pipeline as other models\n",
    "\n",
    "**Callbacks:**\n",
    "- **EarlyStopping**: Monitor `val_accuracy`, patience=10, min_delta=0.0001, restore_best_weights=True\n",
    "- **ReduceLROnPlateau**: Monitor `val_loss`, factor=0.5, patience=7, min_lr=1e-8\n",
    "\n",
    "#### Training Process & Observations\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Low Training Accuracy**: 51.85% (struggles with convergence)\n",
    "- **Low Validation Accuracy**: 60.47% (Updated - slightly better than training)\n",
    "- **Unusual Pattern**: Test accuracy (65.78% - Updated) higher than validation\n",
    "- **Memory Constraints**: Required extensive optimizations\n",
    "- **Largest Model**: 286.5 MB model size\n",
    "\n",
    "**Training Challenges:**\n",
    "1. **Large Feature Dimensions**: 2048 channels create very large attention matrices\n",
    "2. **Memory Limitations**: Required reducing architecture components\n",
    "3. **Convergence Issues**: Model struggles to learn effectively\n",
    "4. **Underfitting**: Low training accuracy suggests model capacity may be insufficient\n",
    "\n",
    "**Why Low Performance:**\n",
    "1. **Reduced Architecture**: Memory optimizations reduced model capacity\n",
    "2. **Large Feature Space**: 2048 dimensions may be too large for transformer to process effectively\n",
    "3. **Limited Transformer Blocks**: Only 2 blocks may be insufficient\n",
    "4. **Batch Size**: Smaller batch size (16) may affect training stability\n",
    "\n",
    "#### Performance Results & Analysis\n",
    "\n",
    "**Training Metrics:**\n",
    "- **Best Epoch**: 20\n",
    "- **Training Accuracy**: 51.85% (low)\n",
    "- **Training Loss**: High (indicating poor fit)\n",
    "\n",
    "**Validation Metrics:**\n",
    "- **Validation Accuracy**: 60.47% (Updated)\n",
    "- **Validation Loss**: High\n",
    "- **Precision**: 53.90%\n",
    "- **Recall**: 60.47% (Updated)\n",
    "- **F1-Score**: 49.20%\n",
    "\n",
    "**Test Metrics (Unseen Data):**\n",
    "- **Test Accuracy**: 65.78% (Updated - unusually higher than validation)\n",
    "- **Test Precision**: 60.08%\n",
    "- **Test Recall**: 60.08%\n",
    "- **Test F1-Score**: 60.08%\n",
    "\n",
    "**Gap Analysis:**\n",
    "- **Overfitting Gap**: -1.64% (Training - Validation)\n",
    "  - Negative gap indicates regularization\n",
    "  - Validation slightly better than training\n",
    "- **Generalization Gap**: -6.59% (Validation - Test)\n",
    "  - **Unusual Pattern**: Test performs better than validation\n",
    "  - May indicate validation set is harder or model benefits from more diverse test data\n",
    "\n",
    "**Model Efficiency:**\n",
    "- **Model Size**: 286.5 MB (largest)\n",
    "- **Parameters**: 75.1M trainable\n",
    "- **Inference Speed**: Slow (large model)\n",
    "- **Memory Usage**: Very high\n",
    "- **Production Ready**: No (low accuracy, large size)\n",
    "\n",
    "#### K-Fold Cross-Validation Results & Analysis\n",
    "\n",
    "**K-Fold Cross-Validation Results:**\n",
    "\n",
    "| Fold | Validation Accuracy | Notes |\n",
    "|------|---------------------|-------|\n",
    "| Fold 1 | 12.53% | Low accuracy |\n",
    "| Fold 2 | 16.20% | Highest accuracy (still very low) |\n",
    "| Fold 3 | 13.61% | Low accuracy |\n",
    "| Fold 4 | 12.53% | Low accuracy |\n",
    "| Fold 5 | 11.90% | Lowest accuracy |\n",
    "\n",
    "**K-Fold Statistics:**\n",
    "- **Average Accuracy**: 13.35% (Very low)\n",
    "- **Standard Deviation**: 1.52% (Low but consistent at low performance)\n",
    "- **Minimum Accuracy**: 11.90%\n",
    "- **Maximum Accuracy**: 16.20%\n",
    "- **Range**: 4.30% (Low accuracy range)\n",
    "\n",
    "**Key Insights from K-Fold Results:**\n",
    "1. **Very Low Performance**: K-Fold average of 13.35% indicates severe training issues\n",
    "2. **Consistent Low Performance**: Standard deviation of 1.52% is low, but this indicates consistently poor performance across all folds\n",
    "3. **Training Problems**: All folds achieved only 11-16% accuracy, suggesting fundamental training or architectural issues\n",
    "4. **Large Gap with Test**: K-Fold average (13.35%) is much lower than test accuracy (65.78%), difference of +52.43%, indicating extreme training instability or configuration differences\n",
    "5. **Architecture Challenges**: Very low K-Fold performance suggests the hybrid architecture may not be suitable or requires significant hyperparameter tuning\n",
    "\n",
    "**Comparison with Other Models:**\n",
    "- K-Fold average (13.35%) is second lowest, indicating severe training issues\n",
    "- Standard deviation (1.52%) is low but this reflects consistently poor performance\n",
    "- Test accuracy (65.78%) is dramatically higher than K-Fold, suggesting training instability or different configurations\n",
    "\n",
    "#### Why This Model Struggles\n",
    "\n",
    "1. **Memory Constraints**: Required reducing architecture capacity\n",
    "2. **Large Feature Dimensions**: 2048 channels create computational challenges\n",
    "3. **Insufficient Capacity**: Reduced transformer blocks may be too limiting\n",
    "4. **Training Instability**: Smaller batch size affects gradient estimates\n",
    "5. **Architecture Mismatch**: ResNet50's 2048 channels may be too large for this hybrid approach\n",
    "\n",
    "#### Recommendations for Improvement\n",
    "\n",
    "1. **Feature Reduction**: Add bottleneck layer to reduce 2048 â†’ 512 before transformer\n",
    "2. **More Transformer Blocks**: Increase to 4 blocks if memory allows\n",
    "3. **Larger Batch Size**: Use gradient accumulation to simulate larger batches\n",
    "4. **Different Architecture**: Consider using ResNet50 features differently (e.g., multi-scale)\n",
    "5. **Progressive Training**: Train transformer components progressively\n",
    "\n",
    "---\n",
    "\n",
    "### 7. ZFNet & Vision Transformer Hybrid Model âš ï¸\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "**Hybrid Architecture (Trained from Scratch):**\n",
    "This model combines a custom ZFNet backbone (trained from scratch) with Vision Transformer. Unlike other models, this doesn't use pre-trained weights, making training more challenging.\n",
    "\n",
    "**Architecture Components:**\n",
    "\n",
    "1. **Feature Extraction Stage (Custom ZFNet Backbone)**:\n",
    "   - **Pre-trained**: âŒ No (trained from scratch)\n",
    "   - **Input**: 224Ã—224Ã—3 RGB images\n",
    "   - **Architecture**: Custom ZFNet with 5 convolutional layers\n",
    "     - **Layer 1**: Conv2D + MaxPooling + BatchNorm\n",
    "     - **Layer 2**: Conv2D + MaxPooling + BatchNorm\n",
    "     - **Layer 3**: Conv2D + MaxPooling + BatchNorm\n",
    "     - **Layer 4**: Conv2D + MaxPooling + BatchNorm\n",
    "     - **Layer 5**: Conv2D + MaxPooling + BatchNorm\n",
    "   - **Output Feature Map**: 7Ã—7Ã—256 spatial feature maps\n",
    "   - **Parameters**: All trainable (no frozen weights)\n",
    "   - **Why Custom ZFNet**: Simpler architecture, smaller feature dimensions (256 channels)\n",
    "\n",
    "2. **Feature Processing Stage (Vision Transformer)**:\n",
    "   - **Patch Creation**: CNN feature maps (7Ã—7Ã—256) reshaped into 49 patches of 256 dimensions\n",
    "   - **Position Embedding**: Learnable positional encodings\n",
    "   - **Class Token**: Prepend learnable classification token\n",
    "   - **Transformer Blocks**: 4 sequential transformer encoder blocks\n",
    "     - **Multi-Head Self-Attention**: 8 attention heads, key_dim = 32 (256/8)\n",
    "     - **MLP**: 2-layer feedforward network with dimension 512 (patch_dim Ã— 2)\n",
    "     - **Layer Normalization**: Pre-norm architecture\n",
    "     - **Residual Connections**: Skip connections\n",
    "   - **Output**: Class token extracted after final transformer block\n",
    "\n",
    "3. **Classification Head**:\n",
    "   - **Dense Layer**: 20 units (one per medicinal plant class)\n",
    "   - **Activation**: Softmax\n",
    "\n",
    "**Total Architecture Parameters:**\n",
    "- **Trainable**: 6.0M parameters (all trainable)\n",
    "- **Non-trainable**: 0 parameters\n",
    "- **Total**: 6.0M parameters\n",
    "- **Model Size**: 22.9 MB (smallest hybrid model)\n",
    "\n",
    "#### Detailed Architecture Flow\n",
    "\n",
    "```\n",
    "Input Image (224Ã—224Ã—3)\n",
    "    â†“\n",
    "Rescaling Layer (included in ZFNet)\n",
    "    â†“\n",
    "Custom ZFNet Backbone (TRAINABLE - from scratch)\n",
    "    â”œâ”€ Conv2D Layer 1 + MaxPool + BatchNorm\n",
    "    â”œâ”€ Conv2D Layer 2 + MaxPool + BatchNorm\n",
    "    â”œâ”€ Conv2D Layer 3 + MaxPool + BatchNorm\n",
    "    â”œâ”€ Conv2D Layer 4 + MaxPool + BatchNorm\n",
    "    â””â”€ Conv2D Layer 5 + MaxPool + BatchNorm\n",
    "    â†“\n",
    "Feature Map (7Ã—7Ã—256)\n",
    "    â†“\n",
    "Reshape to Patches (49 patches Ã— 256 dims)\n",
    "    â†“\n",
    "Add Position Embeddings (learnable)\n",
    "    â†“\n",
    "Prepend Class Token\n",
    "    â†“\n",
    "Transformer Block 1-4\n",
    "    â”œâ”€ Multi-Head Self-Attention (8 heads)\n",
    "    â”œâ”€ Layer Norm + Residual\n",
    "    â”œâ”€ MLP (256 â†’ 512 â†’ 256)\n",
    "    â””â”€ Layer Norm + Residual\n",
    "    â†“\n",
    "Extract Class Token\n",
    "    â†“\n",
    "Dense Classification Layer (20 classes)\n",
    "    â†“\n",
    "Softmax Output\n",
    "```\n",
    "\n",
    "#### Training Configuration & Hyperparameters\n",
    "\n",
    "**Optimizer**: Adam with learning rate 0.0001\n",
    "- Lower LR appropriate for training from scratch\n",
    "- Same as other hybrid models\n",
    "\n",
    "**Loss Function**: Sparse Categorical Crossentropy\n",
    "\n",
    "**Training Setup:**\n",
    "- **Batch Size**: 32 samples per batch\n",
    "- **Total Epochs**: 20 (converged at epoch 17)\n",
    "- **Steps per Epoch**: 65 steps\n",
    "- **Device**: NVIDIA GPU with CUDA support\n",
    "\n",
    "**Data Augmentation**: Same pipeline as other models\n",
    "\n",
    "**Callbacks:**\n",
    "- **EarlyStopping**: Monitor `val_accuracy`, patience=10, min_delta=0.0001, restore_best_weights=True\n",
    "- **ReduceLROnPlateau**: Monitor `val_loss`, factor=0.5, patience=7, min_lr=1e-8\n",
    "\n",
    "#### Training Process & Observations\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Trained from Scratch**: No pre-trained weights (unlike other models)\n",
    "- **Moderate Convergence**: Training accuracy 61.62%, validation 58.91%\n",
    "- **Slight Overfitting**: 2.71% gap (training > validation)\n",
    "- **Good Generalization**: -1.17% gap to test (test > validation)\n",
    "- **Smallest Hybrid**: 22.9 MB, 6.0M parameters\n",
    "- **Unexpected Performance**: Performs worse than standalone ZFNet\n",
    "\n",
    "**Training Challenges:**\n",
    "1. **No Transfer Learning**: Must learn features from scratch\n",
    "2. **Limited Data**: Small dataset makes training from scratch difficult\n",
    "3. **Architecture Complexity**: Hybrid architecture may be too complex for scratch training\n",
    "4. **Feature Learning**: ZFNet must learn good features while transformer processes them\n",
    "\n",
    "**Why Lower Performance:**\n",
    "1. **No Pre-training**: Missing ImageNet-learned features\n",
    "2. **Small Dataset**: Insufficient data for training complex hybrid from scratch\n",
    "3. **Architecture Mismatch**: Transformer may need better features than ZFNet can learn\n",
    "4. **Training Difficulty**: Learning both CNN and transformer simultaneously is challenging\n",
    "\n",
    "#### Performance Results & Analysis\n",
    "\n",
    "**Training Metrics:**\n",
    "- **Best Epoch**: 17\n",
    "- **Training Accuracy**: 61.62%\n",
    "- **Training Loss**: Moderate\n",
    "\n",
    "**Validation Metrics:**\n",
    "- **Validation Accuracy**: 76.74% (Updated)\n",
    "- **Validation Loss**: Moderate\n",
    "- **Precision**: 62.12%\n",
    "- **Recall**: 76.74% (Updated)\n",
    "- **F1-Score**: 54.64%\n",
    "\n",
    "**Test Metrics (Unseen Data):**\n",
    "- **Test Accuracy**: 80.61% (Updated)\n",
    "- **Test Precision**: 60.08%\n",
    "- **Test Recall**: 60.08%\n",
    "- **Test F1-Score**: 60.08%\n",
    "\n",
    "**Gap Analysis:**\n",
    "- **Overfitting Gap**: 2.71% (Training - Validation)\n",
    "  - Positive gap indicates slight overfitting\n",
    "  - Training performs better than validation\n",
    "- **Generalization Gap**: -1.17% (Validation - Test)\n",
    "  - Negative gap indicates good generalization\n",
    "  - Test performs better than validation (unusual but positive)\n",
    "\n",
    "**Model Efficiency:**\n",
    "- **Model Size**: 22.9 MB (smallest hybrid)\n",
    "- **Parameters**: 6.0M trainable\n",
    "- **Inference Speed**: Fast (small model)\n",
    "- **Memory Usage**: Low\n",
    "- **Production Ready**: No (low accuracy)\n",
    "\n",
    "#### K-Fold Cross-Validation Results & Analysis\n",
    "\n",
    "**K-Fold Cross-Validation Results:**\n",
    "\n",
    "| Fold | Validation Accuracy | Notes |\n",
    "|------|---------------------|-------|\n",
    "| Fold 1 | 5.40% | Very low accuracy |\n",
    "| Fold 2 | 4.97% | Very low accuracy |\n",
    "| Fold 3 | 4.10% | Lowest accuracy |\n",
    "| Fold 4 | 5.83% | Highest accuracy (still very low) |\n",
    "| Fold 5 | 4.33% | Very low accuracy |\n",
    "\n",
    "**K-Fold Statistics:**\n",
    "- **Average Accuracy**: 4.93% (Extremely low)\n",
    "- **Standard Deviation**: 0.64% (Low but consistently poor)\n",
    "- **Minimum Accuracy**: 4.10%\n",
    "- **Maximum Accuracy**: 5.83%\n",
    "- **Range**: 1.73% (Extremely low accuracy range)\n",
    "\n",
    "**Key Insights from K-Fold Results:**\n",
    "1. **Extremely Low Performance**: K-Fold average of 4.93% indicates severe training failure\n",
    "2. **Consistently Poor Performance**: Standard deviation of 0.64% is low, but this reflects consistently extremely poor performance across all folds\n",
    "3. **Training Failure**: All folds achieved only 4-6% accuracy (essentially random guessing for 20 classes = 5%), suggesting fundamental training or architectural issues\n",
    "4. **Extreme Gap with Test**: K-Fold average (4.93%) is dramatically lower than test accuracy (80.61%), difference of +75.68%, indicating severe training instability or completely different configurations\n",
    "5. **Architecture Not Suitable**: Extremely low K-Fold performance suggests the ZFNet & ViT hybrid architecture trained from scratch is not suitable for this dataset\n",
    "\n",
    "**Comparison with Other Models:**\n",
    "- K-Fold average (4.93%) is lowest among all models, indicating worst training performance\n",
    "- Standard deviation (0.64%) is low but reflects consistently extremely poor performance\n",
    "- Test accuracy (80.61%) is dramatically higher than K-Fold, suggesting severe training instability or different training configurations\n",
    "\n",
    "#### Why This Model Performs Poorly\n",
    "\n",
    "1. **No Transfer Learning**: Missing pre-trained features significantly hurts performance\n",
    "2. **Small Dataset**: Training hybrid architecture from scratch requires more data\n",
    "3. **Architecture Complexity**: Hybrid may be too complex without pre-training\n",
    "4. **Feature Quality**: ZFNet features may not be rich enough for transformer\n",
    "5. **Training Difficulty**: Learning both components simultaneously is challenging\n",
    "\n",
    "#### Comparison with Standalone ZFNet\n",
    "\n",
    "**Standalone ZFNet performs better (89.73% vs 80.61% - Updated):**\n",
    "- Standalone has 72.0M parameters vs hybrid's 6.0M\n",
    "- Standalone benefits from simpler architecture\n",
    "- Standalone can learn features more effectively without transformer overhead\n",
    "- Hybrid adds complexity without sufficient benefit when trained from scratch\n",
    "\n",
    "---\n",
    "\n",
    "### 8. ZFNet Standalone Model\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "**Standalone CNN Architecture (Trained from Scratch):**\n",
    "This model uses a custom ZFNet architecture trained entirely from scratch. Shows strong regularization effects and performs better than its hybrid counterpart.\n",
    "\n",
    "**Architecture Components:**\n",
    "\n",
    "1. **Custom ZFNet Backbone (All Trainable)**:\n",
    "   - **Pre-trained**: âŒ No (trained from scratch)\n",
    "   - **Input**: 224Ã—224Ã—3 RGB images\n",
    "   - **Architecture**: Custom ZFNet with 5 convolutional layers\n",
    "     - **Layer 1**: Conv2D + MaxPooling + BatchNorm + ReLU\n",
    "     - **Layer 2**: Conv2D + MaxPooling + BatchNorm + ReLU\n",
    "     - **Layer 3**: Conv2D + MaxPooling + BatchNorm + ReLU\n",
    "     - **Layer 4**: Conv2D + MaxPooling + BatchNorm + ReLU\n",
    "     - **Layer 5**: Conv2D + MaxPooling + BatchNorm + ReLU\n",
    "   - **Output**: Feature maps (varies by layer)\n",
    "   - **Parameters**: All trainable (72.0M parameters)\n",
    "   - **Key Features**: \n",
    "     - Batch normalization for stable training\n",
    "     - Max pooling for spatial reduction\n",
    "     - ReLU activation for non-linearity\n",
    "\n",
    "2. **Global Average Pooling (GAP)**:\n",
    "   - **Operation**: Average pooling over spatial dimensions\n",
    "   - **Output**: Feature vector\n",
    "\n",
    "3. **Classification Head**:\n",
    "   - **Dense Layers**: Multiple dense layers for classification\n",
    "   - **Dropout**: Applied for regularization\n",
    "   - **Activation**: Softmax\n",
    "\n",
    "**Total Parameters:**\n",
    "- **Trainable**: 72.0M parameters (all trainable)\n",
    "- **Non-trainable**: 0 parameters\n",
    "- **Total**: 72.0M parameters\n",
    "- **Model Size**: 274.5 MB (very large)\n",
    "\n",
    "#### Detailed Architecture Flow\n",
    "\n",
    "```\n",
    "Input Image (224Ã—224Ã—3)\n",
    "    â†“\n",
    "Rescaling Layer (included in ZFNet)\n",
    "    â†“\n",
    "Custom ZFNet Backbone (ALL TRAINABLE - from scratch)\n",
    "    â”œâ”€ Conv2D Layer 1 + MaxPool + BatchNorm + ReLU\n",
    "    â”œâ”€ Conv2D Layer 2 + MaxPool + BatchNorm + ReLU\n",
    "    â”œâ”€ Conv2D Layer 3 + MaxPool + BatchNorm + ReLU\n",
    "    â”œâ”€ Conv2D Layer 4 + MaxPool + BatchNorm + ReLU\n",
    "    â””â”€ Conv2D Layer 5 + MaxPool + BatchNorm + ReLU\n",
    "    â†“\n",
    "Feature Map\n",
    "    â†“\n",
    "Global Average Pooling\n",
    "    â†“\n",
    "Feature Vector\n",
    "    â†“\n",
    "Dense Layers + Dropout\n",
    "    â†“\n",
    "Softmax Output\n",
    "```\n",
    "\n",
    "#### Training Configuration & Hyperparameters\n",
    "\n",
    "**Optimizer**: Adam with learning rate 0.001 (higher than transfer learning models)\n",
    "- **Rationale**: Training from scratch requires higher learning rate for effective learning\n",
    "- **Beta1**: 0.9, **Beta2**: 0.999\n",
    "\n",
    "**Loss Function**: Sparse Categorical Crossentropy\n",
    "\n",
    "**Training Setup:**\n",
    "- **Batch Size**: 32 samples per batch\n",
    "- **Total Epochs**: 20 (converged at epoch 19)\n",
    "- **Steps per Epoch**: 65 steps\n",
    "- **Device**: NVIDIA GPU with CUDA support\n",
    "\n",
    "**Data Augmentation**: Same pipeline as other models\n",
    "\n",
    "**Callbacks:**\n",
    "- **EarlyStopping**: Monitor `val_accuracy`, patience=5\n",
    "- **ReduceLROnPlateau**: Monitor `val_loss`, factor=0.5, patience=5, min_lr=5e-5\n",
    "\n",
    "#### Training Process & Observations\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Trained from Scratch**: No pre-trained weights\n",
    "- **Higher Learning Rate**: 0.001 (10Ã— higher than transfer learning models)\n",
    "- **Strong Regularization**: Training accuracy (77.97%) much lower than validation (91.09%)\n",
    "- **Large Model**: 72.0M parameters, 274.5 MB\n",
    "- **Better than Hybrid**: Performs better than ZFNet & ViT hybrid (88.21% vs 60.08%)\n",
    "\n",
    "**Training Progression:**\n",
    "- Model showed steady improvement throughout training\n",
    "- Validation accuracy consistently higher than training\n",
    "- Best performance achieved at epoch 19\n",
    "- Strong regularization effects from data augmentation\n",
    "\n",
    "**Why Higher Learning Rate:**\n",
    "1. **Training from Scratch**: Needs larger updates to learn features effectively\n",
    "2. **No Pre-trained Features**: Must learn everything, requiring more aggressive learning\n",
    "3. **Batch Normalization**: Provides stability, allowing higher learning rates\n",
    "4. **Large Model**: More parameters can handle larger learning rates\n",
    "\n",
    "**Why Better than Hybrid:**\n",
    "1. **Simpler Architecture**: Standalone is simpler, easier to train from scratch\n",
    "2. **More Parameters**: 72.0M vs 6.0M allows more capacity\n",
    "3. **Direct Learning**: No transformer overhead, direct feature-to-classification mapping\n",
    "4. **Better Feature Learning**: Can focus on learning good features without transformer constraints\n",
    "\n",
    "#### Performance Results & Analysis\n",
    "\n",
    "**Training Metrics:**\n",
    "- **Best Epoch**: 19\n",
    "- **Training Accuracy**: 77.97%\n",
    "- **Training Loss**: Moderate\n",
    "\n",
    "**Validation Metrics:**\n",
    "- **Validation Accuracy**: 91.09%\n",
    "- **Validation Loss**: Low\n",
    "- **Precision**: 92.25%\n",
    "- **Recall**: 91.09%\n",
    "- **F1-Score**: 90.94%\n",
    "\n",
    "**Test Metrics (Unseen Data):**\n",
    "- **Test Accuracy**: 89.73% (Updated)\n",
    "- **Test Precision**: 89.73% (Updated)\n",
    "- **Test Recall**: 89.73% (Updated)\n",
    "- **Test F1-Score**: 88.21%\n",
    "\n",
    "**Gap Analysis:**\n",
    "- **Overfitting Gap**: -13.12% (Training - Validation)\n",
    "  - Large negative gap indicates very strong regularization\n",
    "  - Validation performs significantly better than training\n",
    "  - Data augmentation makes training harder but improves generalization\n",
    "- **Generalization Gap**: 2.88% (Validation - Test)\n",
    "  - Good generalization (small gap)\n",
    "  - Test accuracy slightly lower than validation (expected)\n",
    "  - Model generalizes well to unseen data\n",
    "\n",
    "**Model Efficiency:**\n",
    "- **Model Size**: 274.5 MB (very large)\n",
    "- **Parameters**: 72.0M trainable\n",
    "- **Inference Speed**: Moderate (large model)\n",
    "- **Memory Usage**: High\n",
    "- **Production Ready**: Moderate (good accuracy but large size)\n",
    "\n",
    "#### K-Fold Cross-Validation Results & Analysis\n",
    "\n",
    "**K-Fold Cross-Validation Results:**\n",
    "\n",
    "| Fold | Validation Accuracy | Notes |\n",
    "|------|---------------------|-------|\n",
    "| Fold 1 | 9.50% | Very low accuracy |\n",
    "| Fold 2 | 20.73% | Highest accuracy (still low) |\n",
    "| Fold 3 | 7.78% | Very low accuracy |\n",
    "| Fold 4 | 7.34% | Lowest accuracy |\n",
    "| Fold 5 | 25.32% | Highest accuracy (still moderate) |\n",
    "\n",
    "**K-Fold Statistics:**\n",
    "- **Average Accuracy**: 14.14% (Very low)\n",
    "- **Standard Deviation**: 7.44% (Very high - Poor consistency)\n",
    "- **Minimum Accuracy**: 7.34%\n",
    "- **Maximum Accuracy**: 25.32%\n",
    "- **Range**: 17.98% (Extremely high variability)\n",
    "\n",
    "**Key Insights from K-Fold Results:**\n",
    "1. **Very Low Performance**: K-Fold average of 14.14% indicates severe training issues\n",
    "2. **Poor Consistency**: Standard deviation of 7.44% is the highest among all models, indicating extreme variability across different data splits\n",
    "3. **High Variability**: Range of 17.98% is extremely high, showing model performance is highly inconsistent across different data splits\n",
    "4. **Extreme Gap with Test**: K-Fold average (14.14%) is dramatically lower than test accuracy (89.73%), difference of +75.59%, indicating severe training instability or different configurations\n",
    "5. **Training Instability**: Very high standard deviation and extreme range suggest model training is highly unstable, with performance varying dramatically based on data split\n",
    "\n",
    "**Comparison with Other Models:**\n",
    "- K-Fold average (14.14%) is very low, indicating poor training performance in K-Fold setting\n",
    "- Standard deviation (7.44%) is highest among all models, showing worst consistency\n",
    "- Test accuracy (89.73%) is dramatically higher than K-Fold, suggesting severe training instability or different training configurations between K-Fold and final training\n",
    "\n",
    "#### Why This Model Shows Strong Regularization\n",
    "\n",
    "1. **Data Augmentation**: Heavy augmentation creates training difficulty\n",
    "2. **Training from Scratch**: Model must learn robust features\n",
    "3. **Large Model**: More parameters benefit from strong regularization\n",
    "4. **Batch Normalization**: Provides additional regularization\n",
    "5. **Dropout**: Applied in classification head\n",
    "\n",
    "#### Comparison with Hybrid Version\n",
    "\n",
    "**Standalone ZFNet (89.73% - Updated) vs ZFNet & ViT Hybrid (80.61% - Updated):**\n",
    "\n",
    "**Advantages of Standalone:**\n",
    "- **Better Accuracy**: 28% higher test accuracy\n",
    "- **Simpler Architecture**: Easier to train from scratch\n",
    "- **More Parameters**: 72.0M vs 6.0M provides more capacity\n",
    "- **Direct Learning**: No transformer overhead\n",
    "\n",
    "**Disadvantages of Standalone:**\n",
    "- **Larger Model**: 274.5 MB vs 22.9 MB (12Ã— larger)\n",
    "- **More Parameters**: 72.0M vs 6.0M (12Ã— more)\n",
    "- **Slower Inference**: Larger model is slower\n",
    "\n",
    "**Why Standalone Performs Better:**\n",
    "1. **Architecture Simplicity**: Simpler architecture is easier to train from scratch\n",
    "2. **Parameter Count**: More parameters allow better feature learning\n",
    "3. **No Transformer Overhead**: Direct CNN-to-classification is more efficient\n",
    "4. **Better Feature Learning**: Can focus solely on learning good CNN features\n",
    "5. **Training Efficiency**: Simpler architecture trains more effectively from scratch\n",
    "\n",
    "---\n",
    "\n",
    "### Common Training Characteristics\n",
    "\n",
    "**Dataset:**\n",
    "- **Total Classes**: 20 Philippine Medicinal Plants\n",
    "- **Image Size**: 224Ã—224Ã—3 (RGB)\n",
    "- **Train/Val/Test Split**: Stratified split ensuring balanced distribution\n",
    "- **Training Steps per Epoch**: 65 steps (consistent across all models)\n",
    "\n",
    "**Data Augmentation (Applied to All Models):**\n",
    "- Random horizontal and vertical flips\n",
    "- Random rotation (Â±15 degrees)\n",
    "- Random zoom (Â±10%)\n",
    "- Random brightness adjustment (Â±10%)\n",
    "- Random contrast adjustment (Â±10%)\n",
    "\n",
    "**Optimization Strategy:**\n",
    "- **Transfer Learning**: Pre-trained ImageNet weights used for MobileNet, VGG16, ResNet, ResNet50\n",
    "- **Frozen Base**: Base model weights frozen, only classifier/top layers trainable\n",
    "- **Learning Rate**: 0.0001 for most models (0.001 for ResNet and ZFNet standalone)\n",
    "- **Learning Rate Scheduling**: ReduceLROnPlateau callback reduces LR by factor of 0.5 when validation loss plateaus\n",
    "\n",
    "**Regularization:**\n",
    "- Data augmentation provides strong regularization (evident from validation > training accuracy in many models)\n",
    "- Early stopping prevents overfitting\n",
    "- Learning rate reduction helps fine-tuning\n",
    "\n",
    "**Hardware:**\n",
    "- **Device**: NVIDIA GPU (CUDA)\n",
    "- **Mixed Precision**: Enabled for ResNet50 & ViT (mixed_float16) to handle large model size\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
